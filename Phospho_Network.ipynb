{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational phospho-proteomic network inference pipeline\n",
    "####  by Matt Macgilvary\n",
    "\n",
    "###### This pipeline turns a list of S. cerevesiae phospho-peptides that exhibit stress responsive abundance changes, as measured by mass spectrometry, into a hierarchical signaling network, connecting upstream kinases and phosphatases to their downstream targets. Our computational pipeline is based on the premise that kinases and phosphatases recognize target substrates through specific amino acid sequences at the phosphorylated residue, called phosphorylation motifs. This pipeline groups phospho-peptides with similar abundance changes and the same phosphorylation motif into modules. Modules are partitioned into smaller groups, called submodules, based on differences in phospho- peptide abundance in mutant strain(s) (sources). Candidate submodule regulators, called shared interactors, are identified through enrichment analysis using a protein interaction network in yeast (Chasman et al., 2014). Shared interactor-submodule pairs serve as inputs for a previously developed Integer Programming (IP) approach that connects the sources to their downstream target submodules (Chasman et al., 2014).\n",
    "\n",
    "###### Please see our bioRxiv preprint for additional information:\n",
    "    Network inference reveals novel connections in pathways regulating growth and defense in the yeast salt response.   Matthew E. MacGilvray+, Evgenia Shishkova+, Deborah Chasman, Michael Place, Anthony Gitter, Joshua J. Coon, Audrey P. Gasch. bioRxiv 2017. doi:10.1101/176230\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "### The user should define differentially changing phospho-peptides in the \"WT\" or \"Parent\" strain using their own criteria (eg; fold-change, p-value, etc.), followed by grouping/clustering phospho-peptides based on similar directionality of abundance change.\n",
    "\n",
    "\n",
    "# Identify motifs by Calling Motifx.py\n",
    "    \n",
    "    This automates submitting jobs to the Motif-x Website (http://motif-x.med.harvard.edu/)\n",
    "\n",
    "### Expected Input  : A single Plain text file listing excel files to process, one excel file name per line.\n",
    "\n",
    "    text file:\n",
    "    data_sheet1.xlsx\n",
    "    data_sheet2.xlsx\n",
    "\n",
    "\tThe Excel file format:\n",
    "\tPpep\tGroup\tLocalized_Sequence\tMotif_X_Input_Peptide\n",
    "\tYGL076C_T8_S11\tInduced\tAAEKILtPEsQLKK\tAAEKILT*PES*QLKK\n",
    "\n",
    "\tColumn order is unimportant, column names must match above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i 'Motifx.py' -f 'inputfiles' -u 'reference/orf_trans_all.20150113.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import Bio\n",
    "import glob\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from scipy.stats import hypergeom\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "from Bio.Alphabet import IUPAC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep input for  Identify Modules and Submodules \n",
    "\n",
    " Look in your directory for a file called *-Motifx-results.txt,  This will be modified for use as input in the next step.\n",
    " The input file, which is in .csv format, must use the following format:\n",
    "\n",
    "### Column headers\n",
    "    Ppep, Cluster, Motif, Peptide, FirstMutantNamePhenotype, SecondMutantNamePhenotype, ThirdMutantNamePhenotype\n",
    "### Ppep = phospho-peptide, YORF followed by the phosphorylated residue(s). \n",
    "    Examples - YLR113W_S115, YLR113W_S115_T179\n",
    "### Cluster - either 'Induced' or 'Repressed'\n",
    "### Motif - Identified by Motif-X\n",
    "### Peptide - 13 aa long phospho-peptide returned by Motif-X. The middle residue is the phosphorylated amino acid.\n",
    "### Example Phenotype mutants, these are the last 3 columns and are gene names.\n",
    "\n",
    " FirstMutantNamePhenotype -  'hog1', the name of a gene for which we interogated a deletion strain by phospho-proteomics\n",
    " SecondMutantNamePhenotype - 'pde2' , the name of a gene for which we interogated a deletion strain by phospho-proteomics\n",
    " ThirdMutantNamePhenotype - 'cdc14' , the name of a gene for which we interogated a deletion strain by phospho-proteomics\n",
    "\n",
    "### Example  input file generated from the Motifx output:\n",
    "\n",
    "Ppep,Cluster,Motif,Peptide,hog1,pde2,cdc14\n",
    "YLR319C_T169,Induced,......TP.....,EGTREGTPLSSRK,Induced_Defective,,Repressed_Amplified\n",
    "YJL070C_T195,Induced,......TP.....,TGTGAATPHRHGY,Induced_Defective,,Repressed_Amplified\n",
    "YAL035W_T390,Induced,......TP.....,AATPAATPTPSSA,Induced_Defective,,Repressed_Amplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This can be used to generate your file automatically.\n",
    "# Replace these names w/ your mutants of interest\n",
    "# replace output.write gene names w/ your gene names of interest\n",
    "mutant1 = 'Induced_Defective'\n",
    "mutant2 = ''\n",
    "mutant3 = 'Repressed_Amplified'\n",
    "\n",
    "with open('idModules.csv', 'w') as output:       # open identify_modules_and_submodules_inputfile for writing\n",
    "    output.write('Ppep,Cluster,Motif,Peptide,hog1,pde2,cdc14\\n')        # replace gene names here\n",
    "    with open('motifx_sample-Motifx-results.txt') as f:      \n",
    "        for line in f:\n",
    "            row = line.rstrip().split(',')\n",
    "            row.append(row.pop(1))\n",
    "            row.append(mutant1)\n",
    "            row.append(mutant2)\n",
    "            row.append(mutant3)\n",
    "            output.write(','.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run  Identify_Modules_and_Submodules step.\n",
    "\n",
    " This script identifies co-regulated groups of phospho-peptides using the following approach:\n",
    " \n",
    "  1) First, the script identifies 'modules', which are groups of phospho-peptides that exhibit the same directionality in stress-dependent abundance change (ie, increased 'Induced', or decreased 'Repressed') and the same motif.\n",
    " The module nomenclature is as follows: Induced/Repressed- motif (ex: Induced..RK.s....).\n",
    " \n",
    "  2) Next, the script partitions modules into 'submodules' based on their phospho-peptide constituents dependency on a protein(s) for stress-dependent abundance changes (ie, phospho-peptides that exhibit increased 'amplified' or decreased 'defective' abundance in a deletion strain compared to the 'WT' or  'Parental' type strain). These phenotypes are user defined. If two or more mutant phenotypes are recorded for a phospho-peptide then it's placed into two separate subModules (one for each mutant phenotype). If there was not a mutant phenotype at a user defined threshold then the phenotype is 'No-Phenotype'\n",
    " \n",
    " The submodule nomenclature is as follows: module name-mutant phenotype/No-Phenotype (ex: Induced..RK.s....Mutant_Defective).\n",
    "\n",
    " Possible submodule phenotypes: Induced-Defective, Induced-Amplified, Repressed-Defective, Repressed-Amplified, Induced-No-Phenotype, Repressed-No-Phenotype \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required python libraries\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.stats import hypergeom\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "Data=pd.read_csv('idModules.csv') # Define path to input file\n",
    "\n",
    "def Slicedataframe():\n",
    "    '''Define a function that slices the input dataframe into independent dataframes based on the Cluster names. Next, slice these dataframes based on the presence of the same motif, generating 'modules' '''  \n",
    "    ClusterLST=Data['Cluster'].unique().tolist()                            # generate a list of unique Cluster names (ie, 'Induced' and 'Repressed')\n",
    "    lst=[]                                                                 \n",
    "    DF=Data.copy()                                                         \n",
    "    for cluster in ClusterLST:                                              # Select the first 'cluster' on the list \n",
    "        DF2=DF.loc[DF['Cluster']== cluster]                                 # Create a new dataframe by selecting only those rows that contain the selected 'cluster' in the 'Cluster' column \n",
    "        MotifLST=DF2['Motif'].unique().tolist()                             # From the newly created dataframe, place each instance of a unique motif into a list\n",
    "        cleanedMotifLST = [x for x in MotifLST if str(x) != 'nan']          # drop the string 'nan' from the list. 'nan' occurs for Ppeps that did not have an identified Motif from Motif-X. \n",
    "        for motif in cleanedMotifLST:                                       # Select a motif in the list\n",
    "            DF3=DF2.loc[DF['Motif']== motif]                                # Filter the dataframe, selecting only those rows that contain 'motif' in the Motif column\n",
    "            DF3['freq'] = DF3.groupby('Motif')['Motif'].transform('count')  # Produce a new column, called 'freq' that contains the number of rows, and thus phospho-peptides, that contain a given motif.\n",
    "            lst.append(DF3) \n",
    "        \n",
    "    return lst\n",
    "\n",
    "SlicedDF_lst=Slicedataframe()\n",
    "\n",
    "def ConcatenateDFs():\n",
    "    ''' Define a function that appends the dataframes in the SlicedDF_list together. '''\n",
    "    EmptyDF = pd.DataFrame()                                                # create an empty dataframe\n",
    "    for df in SlicedDF_lst:                                                \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)                                          # append to the empty DF the dataframe selected and overwrite the empty dataframe\n",
    "    return EmptyDF\n",
    "\n",
    "Final_DF=ConcatenateDFs()\n",
    "FinalDFV2=Final_DF.fillna(0)                                                #  fill any NaN values with '0'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "def Module_Motif_NoMutantPhenotypeExists(df):\n",
    "    ''' Define a function that assigns no-phenotype submodules'''\n",
    "    if (df['hog1']==0) & (df['pde2']==0) & (df['cdc14']==0):\n",
    "        return 'No_Phenotype_Exists'\n",
    "    \n",
    "FinalDFV2['Phenotype']=FinalDFV2.apply(Module_Motif_NoMutantPhenotypeExists, axis=1) \n",
    "FinalDFV2=FinalDFV2.loc[FinalDFV2['Phenotype']=='No_Phenotype_Exists']        # Select all rows for which \"No_Phenotype_Exists\" in the 'Phenotype' column.\n",
    "FinalDFV2['subModule']=FinalDFV2.Cluster.map(str) + \"_\" + FinalDFV2.Motif + \"_\" + FinalDFV2.Phenotype                                   # create a new column, called submodule, that contains the concatenated strings in the 'Cluster', 'Motif', and 'Phenotype' columns.\n",
    "\n",
    "# CHANGE GENE NAMES HERE\n",
    "FinalDF=Final_DF.dropna(subset = ['hog1', 'pde2', 'cdc14'], how='all')        # Remove rows that have NaN in all 3 columns representing mutant phenotpes. This steps removes theNo-phenotype submodules which were creat                                                                               # ed above. \n",
    "FinalDF=FinalDF.fillna(0)                                                     # fill any NaN that remain with '0'\n",
    "lstCols=['hog1', 'pde2', 'cdc14']                                             # make a list that contains the column headers for the 3 mutants. \n",
    "\n",
    "\n",
    "\n",
    "def DefineMutantContribution(row):\n",
    "    ''' Define a function that identifies for each phospho-peptide if it has a phenotype in more than one mutant strain'''\n",
    "    dictData={} \n",
    "    for colname in lstCols:    \n",
    "        if not row[colname]==0:                                                # if value is not equal to zero, there is a mutant phenotype (ex; Induced_defective)\n",
    "            dictData[colname]=row[colname]  \n",
    "    if len(dictData.keys())==0: return 0  \n",
    "    else:\n",
    "        return \":\".join(dictData.keys())\n",
    "    \n",
    "FinalDF['Contribution']=FinalDF.apply(lambda x: DefineMutantContribution(x), axis=1) \n",
    "\n",
    "\n",
    "\n",
    "def DefinePhenotypeFromMutants(row):\n",
    "    ''' Define a function that captures the mutant phenotype for Ppeps with multiple phenotypes and places it within a column'''\n",
    "    dictData={}  \n",
    "    for colname in lstCols:   \n",
    "        if not row[colname]==0:\n",
    "            dictData[colname]=row[colname] \n",
    "    if len(dictData.keys())==0: return 0 \n",
    "    else:\n",
    "        return \":\".join(dictData.values()) \n",
    "    \n",
    "FinalDF['Phenotype']=FinalDF.apply(lambda x: DefinePhenotypeFromMutants(x), axis=1)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Determine all Ppeps that have 2 or more mutant phenotypes (Pde2/Hog1/Cdc14 phenotypes), then the script produces a new column with individual subModule names for \n",
    "the hog1 phenotype''' \n",
    "\n",
    "FinalDF_multiplePhenotypes=FinalDF[FinalDF['Contribution'].str.contains(\":\")]     # Select 'contribution column rows that contain \":\", which means the Ppep has two mutant phenotypes since this is a separator between gene names\n",
    "FinalDF_multiplePhenotypes_hog1=FinalDF_multiplePhenotypes[FinalDF_multiplePhenotypes['Contribution'].str.contains(\"hog1\")] \n",
    "FinalDF_multiplePhenotypes_hog1['Hog1']='hog1' \n",
    "FinalDF_multiplePhenotypes_hog1['subModule']=FinalDF_multiplePhenotypes_hog1.Cluster.map(str) + \"_\" + FinalDF_multiplePhenotypes_hog1.Motif + \"_\" + FinalDF_multiplePhenotypes_hog1.Hog1 + \"_\" + FinalDF_multiplePhenotypes_hog1.hog1\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Define all Ppeps that have 2 or more mutant phenotypes (Pde2/Hog1/Cdc14 phenotypes), then the script produces a new column with individual subModule names for \n",
    "the pde2 phenotype''' \n",
    "FinalDF_multiplePhenotypes_pde2=FinalDF_multiplePhenotypes[FinalDF_multiplePhenotypes['Contribution'].str.contains(\"pde2\")]\n",
    "FinalDF_multiplePhenotypes_pde2['Pde2']='pde2'\n",
    "FinalDF_multiplePhenotypes_pde2['subModule']=FinalDF_multiplePhenotypes_pde2.Cluster.map(str) + \"_\" + FinalDF_multiplePhenotypes_pde2.Motif + \"_\" + FinalDF_multiplePhenotypes_pde2.Pde2 + \"_\" + FinalDF_multiplePhenotypes_pde2.pde2\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Define all Ppeps that have 2 or more mutant phenotypes (Pde2/Hog1/Cdc14 phenotypes), then the script produces a new column with individual subModule names for \n",
    "the cdc14 phenotype'''\n",
    "\n",
    "FinalDF_multiplePhenotypes_cdc14=FinalDF_multiplePhenotypes[FinalDF_multiplePhenotypes['Contribution'].str.contains(\"cdc14\")]\n",
    "FinalDF_multiplePhenotypes_cdc14['Cdc14']='cdc14'\n",
    "FinalDF_multiplePhenotypes_cdc14['subModule']=FinalDF_multiplePhenotypes_cdc14.Cluster.map(str) + \"_\" + FinalDF_multiplePhenotypes_cdc14.Motif + \"_\" + FinalDF_multiplePhenotypes_cdc14.Cdc14 + \"_\" + FinalDF_multiplePhenotypes_cdc14.cdc14\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "'''This section of code appends the above mutant dataframes together (ie, FinalDF_multiplePhenotypes_cdc14, etc.) (contained \":\"). The result is Ppeps with phenotypes in more than one strain are listed on multiple lines rather than a single line'''\n",
    "\n",
    "FinalDF_mutants=FinalDF_multiplePhenotypes_cdc14.append(FinalDF_multiplePhenotypes_hog1) \n",
    "FinalDF_mutants_Final=FinalDF_mutants.append(FinalDF_multiplePhenotypes_pde2)\n",
    "\n",
    "FinalDF_mutants_Final=FinalDF_mutants_Final[['Ppep','Cluster','Motif','Peptide','hog1','pde2','cdc14','freq','Contribution','Phenotype','subModule']] # Only retain these columns \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Drop from the original dataframe rows containing Ppeps with multiple mutant phenotypes. \n",
    "FinalDF_minus_multiPhenotypePpeps=FinalDF[FinalDF.Contribution.str.contains(\":\")==False] # Removing all rows that contain \":\", and thus are phospho-peptides with multiple mutant phenotypes\n",
    "\n",
    "\n",
    "# Generate the final submodule names\n",
    "FinalDF_minus_multiPhenotypePpeps['subModule']=FinalDF_minus_multiPhenotypePpeps.Cluster.map(str) + \"_\" + FinalDF_minus_multiPhenotypePpeps.Motif + \"_\" + FinalDF_minus_multiPhenotypePpeps.Contribution + \"_\" + FinalDF_minus_multiPhenotypePpeps.Phenotype\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Ppeps_with_PhenotypesDF=FinalDF_minus_multiPhenotypePpeps.append(FinalDF_mutants_Final)  # Appending together the dataframes that originally had single mutant phenotypes, and the dataframe that started with multiple mutant Phentoypes, but now contains single listings for each Ppep-mutant phenotype\n",
    "\n",
    "\n",
    "\n",
    "# Remove any submodule that only has a single Ppep constituent, since by default a submodule must contain 2 Ppeps. \n",
    "Ppeps_with_PhenotypesDF_subModules=Ppeps_with_PhenotypesDF[Ppeps_with_PhenotypesDF.duplicated(['subModule'], keep='last') | Ppeps_with_PhenotypesDF.duplicated(['subModule'])]  # only retain duplicates, get rid of single entries \n",
    "\n",
    "\n",
    "\n",
    "# Append to the dataframe with phenotype subModules, all No-Phenotype submodules\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF=Ppeps_with_PhenotypesDF_subModules.append(FinalDFV2) # append to dataframe\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF=Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF[['Ppep', 'Cluster', 'Motif', 'Peptide', 'hog1', 'pde2', 'cdc14', 'freq', 'Contribution', 'Phenotype', 'subModule']]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Create a column with the 'Module' name '''\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF['Module']=Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF.Cluster.map(str) + \"_\" + Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF.Motif \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# define a function that will write out a dataframe as a tab separated file\n",
    "def Dataframe_to_Tsv (dataframe, NewFileName):\n",
    "    path =\"/home/mplace/projects/forMatt/Phospho_Network/\"\n",
    "    dataframe.to_csv (path+NewFileName,sep='\\t')\n",
    "\n",
    "Dataframe_to_Tsv(Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF, 'Modules_pPep.csv') \n",
    "# The above file contains all modules and subModules with and without mutant phenotypes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Shared Interactors \n",
    "\n",
    "''' This script identifies proteins enriched for interactions with Submodule constituent proteins, based on known interactions in the background network. We call these proteins 'Shared Interactors'. The background network is a protein\n",
    "interaction network curated in yeast under mostly nutrient replete conditions that contains 4638 proteins and ~ 25,000 interactions, including directed (ex; kinase-substrate), and \n",
    "non-directed. \n",
    "\n",
    "Proteins enriched for interactions with Submodule proteins at a 5% FDR, determined by a hypergeometric test and BH correction, are considered shared interactors.\n",
    "\n",
    "Shared Interactors represent numerous functional classes, including kinases and phosphatases. Kinase and phosphatase shared interactors represent potential Submodule regulators.\n",
    " \n",
    "HyperG function:\n",
    "distrib=hypergeom(N,M,n)\n",
    "distrib.pmf(m)\n",
    "\n",
    "N - population size (4638 unique proteins in Background network file - phospho_v4_bgnet_siflike_withdirections_Matt_Modified.csv)\n",
    "\n",
    "M - total number of successes  (# of interactions for a given protein. ie. Protein A has 200 known interactions in the background network).\n",
    "\n",
    "n - the number of trials (also called sample size) -  ie. (Number of proteins that reside within a submdoule)\n",
    "\n",
    "m - the number of successes - for example: Protein A, a shared interactor, has 35 interactions with proteins in Submodule B. \n",
    "\n",
    "input file : \n",
    "Submodule,ORF\n",
    "Induced_......TP....._cdc14_Repressed_Amplified,YLR319C\n",
    "Induced_......TP....._cdc14_Repressed_Amplified,YJL070C\n",
    "\n",
    "#### NOTE: ORF names have need to have the '-' removed,  YER074W-A becomes YER074A.\n",
    "\n",
    "The other input files are provided.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# create the input file based on the output of the previous step.\n",
    "with open('Submodule_constituents.csv', 'w') as out:\n",
    "    out.write('Submodule,ORF\\n')\n",
    "    with open('Modules_pPep.csv') as f:\n",
    "        f.readline()                            # skip header\n",
    "        for line in f:\n",
    "            data = line.rstrip().split('\\t')\n",
    "            name = data[1].split('_')[0]   \n",
    "            name = re.sub('-', '', name)\n",
    "            row = data[-2] + ',' + name + '\\n'\n",
    "            out.write(row)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Submodule_DF   = pd.read_csv(current_dir + '/Submodule_constituents.csv')                                                                       # File that contains Submodule names and their protein constituents\n",
    "BgNet          = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Background_Network.csv')                                                                                   # Background network of protein interactions\n",
    "Num_Prot_Inter = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Number_Interactions_Each_Protein.csv')                                              # Number of protein interactions for each protein in the background network\n",
    "Annotation_DF  = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Annotation.csv')                                                   # Yeast protein annotation file\n",
    " \n",
    "Submodule_List=Submodule_DF['Submodule'].unique().tolist()                                                                                                  # Send the Submodules to a list, but filter out duplicates, which there will be many, since the Submodules will have been found in many proteins.\n",
    "\n",
    "dicOrfs={}\n",
    "for Submodule in Submodule_List:                                                                                                                            # Key (Submodule), Value (Yeast ORFs that are Submodule constituents). Filter ORFs found twice to single occurence (important for enrichment analysis)\n",
    "    dicOrfs[Submodule]=(Submodule_DF.loc[Submodule_DF['Submodule'] == Submodule])['ORF'].unique().tolist()\n",
    "        \n",
    "\n",
    "dicOrfsCounts={}  \n",
    "for k,v in dicOrfs.items():  \n",
    "    if k not in dicOrfsCounts:  \n",
    "        value=len(v)            \n",
    "        dicOrfsCounts[k]=value\n",
    "        \n",
    "df_Submodule_Size=pd.DataFrame(list(dicOrfsCounts.items()),                                                                                                  # convert dict to dataframe.\n",
    "                      columns=['Submodule','n'])\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#def DF_to_TSV(dataframe, NewFileName): \n",
    "#    ''' Define a function that writes out a dataframe as TSV'''\n",
    "#    path ='/Users/mmacgilvray18/Desktop/' \n",
    "#    dataframe.to_csv (path+NewFileName,sep='\\t')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def SliceDataframe():\n",
    "    ''' For each Submodule identify all proteins that interact with the Submodule proteins in the backgroudn network '''\n",
    "    lst = []\n",
    "    for key in dicOrfs.keys():                                                                                                                             #Select the key, which is a Submodule, from the dict\n",
    "        CurrentDF=BgNet.copy() \n",
    "        x=CurrentDF[CurrentDF['Protein1'].isin(dicOrfs[key])].rename(columns={'Protein1':'Submodule_Containing_Proteins', 'Protein2':'Possible_Shared_Interactors'})                              #Create a new dataframe that is a slice of the salt background network, and only contains proteins that were passed in \"dicOrfs[key]\". At the same time, rename the columns                                \n",
    "        x['Submodule']=key \n",
    "        lst.append(x)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Sliced_dataframe_list= SliceDataframe()\n",
    "    \n",
    "      \n",
    "def Add_n():    \n",
    "    ''' Function adds 'n', the number of proteins in the Submodule, to each dataframe'''\n",
    "    lst= []\n",
    "    for df in Sliced_dataframe_list:\n",
    "        NewDF=df.merge(df_Submodule_Size)\n",
    "        lst.append(NewDF)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Sliced_dataframe_list= Add_n()\n",
    "\n",
    "def Identify_Shared_Interactors():\n",
    "    ''' Function identifies proteins that interact with at least 2 protein constituents of each submodule'''\n",
    "    \n",
    "    lst=[] \n",
    "    for df in Sliced_dataframe_list: \n",
    "        NewDF=df.copy()\n",
    "        NewDF2=NewDF[NewDF.duplicated(['Possible_Shared_Interactors'], keep = 'last')| NewDF.duplicated(['Possible_Shared_Interactors'])]                  # Only retain proteins that interact with at least 2 submodule protein constituents\n",
    "        x=NewDF2.sort_values(by='Possible_Shared_Interactors', ascending=True) \n",
    "        lst.append(x)\n",
    "       \n",
    "    return lst\n",
    "\n",
    "Shared_Interactors_lst=Identify_Shared_Interactors()\n",
    "\n",
    "\n",
    "def AppendDFs_that_Contain_AllSharedInteractors_and_their_targets():\n",
    "    ''' Function appends all submodules and their shared interactors together into a single file'''\n",
    "    EmptyDF = pd.DataFrame() \n",
    "    for df in Shared_Interactors_lst:  \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)\n",
    "    return EmptyDF\n",
    "\n",
    "SI_andTargets=AppendDFs_that_Contain_AllSharedInteractors_and_their_targets()\n",
    "\n",
    "\n",
    "SI_andTargets_FINAL=pd.merge(left=SI_andTargets, right=Annotation_DF, how='left',\n",
    "                              left_on='Possible_Shared_Interactors', right_on='systematic_name_dash_removed')                                               # complete a merge so I can get the dashes back in the names, which are not included in the background network\n",
    "del SI_andTargets_FINAL['Possible_Shared_Interactors']                                                                                                      # drop because  lacks the dashes which are needed for the correct naming convention\n",
    "del SI_andTargets_FINAL['systematic_name_dash_removed']                                                                                                     # drop because carried over from the merge\n",
    "del SI_andTargets_FINAL['Directed']\n",
    "\n",
    "SI_andTargets_FINAL.columns = ['Submodule_Containing_Proteins', 'Interaction', 'Submodule', 'n','Possible_Shared_Interactors']                        # rename columns\n",
    "\n",
    "\n",
    "myDF = pd.DataFrame(SI_andTargets_FINAL)\n",
    "# output name for shared interactors\n",
    "filename = 'SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv'\n",
    "myDF.to_csv(filename, index=False, encoding='utf-8' )              # All interactions between SIs and their submodule constituent proteins. No enrichment at this step.\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Preparing dataframe for Hypergeometric test'''\n",
    "\n",
    "def Add_N_and_m():\n",
    "    ''' Function adds 'N' and calculates 'm' values, which are inputs for the hypergeometric test, to the datframe'''\n",
    "    lst=[]\n",
    "    for df in Shared_Interactors_lst:\n",
    "        NewDF=df.copy()\n",
    "        NewDF['N'] = 4638                                                                                                                                   # of proteins in the background network\n",
    "        NewDF['m'] = NewDF.groupby('Possible_Shared_Interactors')['Possible_Shared_Interactors'].transform('count')\n",
    "        lst.append(NewDF)\n",
    "    \n",
    "    return lst\n",
    "\n",
    "Dataframes_list_with_n_N_m=Add_N_and_m()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def Drop_dups():\n",
    "    ''' For each dataframe, which contains a single submodule, it's protein constituents, and shared interactors, drop duplicate entries for identified SI proteins\n",
    "    . This leaves a single entry for each shared interactor protein. '''\n",
    "    lst=[]\n",
    "    for df in Dataframes_list_with_n_N_m:\n",
    "        NewDF=df.copy()\n",
    "        Final_DF=NewDF.drop_duplicates('Possible_Shared_Interactors')\n",
    "        Final_DF=Final_DF.rename(columns={'Possible_Shared_Interactors':'Shared_Interactor'})\n",
    "        lst.append(Final_DF)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Drop_Dups_lst=Drop_dups()\n",
    "\n",
    "\n",
    "def Return_M():\n",
    "    ''' Function identifies 'M' (the total number of interactions for each Shared Interactor protein in the background network) and adds that number\n",
    "    to the dataframe'''\n",
    "    lst=[]\n",
    "    for df in Drop_Dups_lst:\n",
    "        NewDF=df.copy()\n",
    "        NewDF2=df.copy()\n",
    "        NewDF_lst=NewDF['Shared_Interactor'].tolist()                                                                                                            # place all proteins in the 'Shared_Interactor' column in a list \n",
    "        Shared_Interactors=Num_Prot_Inter[Num_Prot_Inter['Protein'].isin(NewDF_lst)].rename(columns={'Protein':'Shared_Interactor', 'Total':'M'})\n",
    "        Shared_Interactor_merge=Shared_Interactors.merge(NewDF2, on='Shared_Interactor')\n",
    "        Shared_Interactor_merge=Shared_Interactor_merge.sort_values(by='Shared_Interactor', ascending=True)\n",
    "        lst.append(Shared_Interactor_merge)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Return_M_lst=Return_M()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "def hyper(N,M,n,m): \n",
    "    ''' Function defines the parameters for a hypergeometric test that returns a p-value representing the chances of identifying >= x, where x is the number of successes '''  \n",
    "    frozendist=hypergeom(N,M,n)\n",
    "    ms=np.arange(m, min(n+1, M+1))\n",
    "    rv=0;\n",
    "    for single_m in ms: rv=rv+frozendist.pmf(single_m)\n",
    "    return rv\n",
    "\n",
    "def run_hyper():\n",
    "    ''' Function calls the hypergeometric function above  on each shared interactor for each submodule'''\n",
    "    lst=[]\n",
    "    for df in Return_M_lst:\n",
    "        if not df.empty:\n",
    "            NewDF=df.copy()\n",
    "            NewDF['p-value'] = NewDF.apply(lambda row: hyper(row['N'], row['M'], row['n'], row['m']), axis=1)\n",
    "            lst.append(NewDF)\n",
    "        \n",
    "    return lst \n",
    "\n",
    "run_hyper_lst=run_hyper()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def AppendDFs():\n",
    "    ''' Append DFs for each submodule and it's SIs together into a single DF'''   \n",
    "    EmptyDF = pd.DataFrame() #\n",
    "    for df in run_hyper_lst: \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)\n",
    "    return EmptyDF\n",
    "\n",
    "Final=AppendDFs()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Prepping for Benjamini Hochberg procedure. Below code is ranking p-values from 1 to n based on lowest to highest p-value score'''\n",
    "\n",
    "Final=Final.sort_values(by=['p-value'],ascending=[True])                                                                                              # Sort p-values from lowest to highest\n",
    "Final_resetIndex=Final.reset_index()                                                                                                        # Reset the index after the sort\n",
    "Final_resetIndex.index +=1                                                                                                                  # start numbering at 1 for index\n",
    "       \n",
    "NewDF=Final_resetIndex\n",
    "NewDF_Allp_values=Final_resetIndex\n",
    "NewDF=NewDF[['p-value']]                                                                                                                    # select only the p-value column of the dataframe \n",
    "NewDF_dropdups=NewDF.drop_duplicates('p-value')                                                                                             # drop duplicate p-values\n",
    "NewDF_dropdups=NewDF_dropdups.reset_index()                                                                                                 # reset the index\n",
    "NewDF_dropdups.index +=1                                                                                                                    # start numbering at 1 for index\n",
    "NewDF_dropdups['Rank(i)'] = NewDF_dropdups.index                                                                                            # #Add a rank column that will be filled with index values. \n",
    "NewDF_dropdups=NewDF_dropdups.drop('index', 1)                                                                                              # Drop the additional column 'index' that is not sorted.\n",
    "NewDF_merge=NewDF_Allp_values.merge(NewDF_dropdups, on='p-value')                                                                           # create a new dataframe that is a merge of the dataframe with all p-values, and the dataframe with unique p-values and their ranks. \n",
    "NewDF_merge=NewDF_merge.drop('index',1)                                                                                                     # drop the index that was added from the merge. This leaves all p-values ordered from lowest to highest with their ranking.\n",
    "\n",
    "'''Add parameters necessary for completing Benjamini-Hochberg procedure '''\n",
    "\n",
    "NewDF=NewDF_merge\n",
    "NewDF['m_(number_of_tests)']=(len(NewDF))                                                                                                   # Add 'm (number of tests)' column \n",
    "NewDF['Q_(FDR)']=0.05                                                                                                                       # Add Q (FDR) column. This can be changed manually.\n",
    "NewDF['(i/m)Q']=((NewDF['Rank(i)']/NewDF['m_(number_of_tests)'])*NewDF['Q_(FDR)'])                                                          # add the (i/m)Q column \n",
    "NewDF['BH_significant']=NewDF.apply(lambda x: 1 if x['p-value']<x['(i/m)Q'] else 0, axis=1)                                                 # Identify which proteins are  significant. \n",
    "NewDF=pd.merge(left=NewDF, right=Annotation_DF, how='left', left_on='Shared_Interactor', right_on='systematic_name_dash_removed')           # complete a merge to recover dashed version of YORFs\n",
    "del NewDF['Shared_Interactor'] \n",
    "del NewDF['systematic_name_dash_removed']\n",
    "del NewDF['Directed']\n",
    "NewDF.columns = ['M','Submodule_Containing_Proteins', 'Interaction', 'Submodule', 'n','N','m','p-value','Rank(i)', 'm_(number_of_tests)', 'Q_(FDR)','(i/m)Q','BH_significant', 'Shared_Interactor'] # rename columns\n",
    "\n",
    "myDF = pd.DataFrame(NewDF)\n",
    "filename = 'Network_Submodule_Nodes_background_Network.csv'\n",
    "myDF.to_csv(filename, index=False, encoding='utf-8' )       # Write out final file with enriched shared interactors for each submodule\n",
    "\n",
    "\n",
    "\n",
    "# FILTER FOR THE FINAL Shared Interactors.\n",
    "# Open and parse Network_Submodule_Nodes_background_Network.csv \n",
    "# Only keep the identified Shared Interactors about the first zero that appears in the BH_Significant column.\n",
    "with open('Final_enriched.csv', 'w') as outfile, open('Network_Submodule_Nodes_background_Network.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('M,'):\n",
    "            outfile.write(line)\n",
    "            continue\n",
    "        dat = line.split(',')\n",
    "        if dat[12] == '0':\n",
    "            break\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "f.close()\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Shared Interactors Inputs Outputs\n",
    "\n",
    "The function of this script is as follows: For each SI and it's connections with submodule protein constituents, determine if the SI acts upon the submodule (that is, the Shared Interactor has at least 1 directional interaction, or ppi interaction, with a submodule protein), or if the submodule acts upon the SI (that is, all interactions between the SI and submodule proteins have the 'Reverse' designation', indicating that the submodule proteins act upon the SI).\n",
    "\n",
    "-If all of the interactions are reversed, then the script will define the relationship between the SI and the submodule as \"Output\"\n",
    "\n",
    "-If there is at least one interaction that is directed from SI towards submodule, or is a ppi, the relationship between the SI and the submodule is defined as \"Input\"\n",
    "\n",
    "This script takes an input file that contains the following:\n",
    "\n",
    "- All enriched Shared Interactors (SIs) (according to HyperG) and their connections to submodules.\n",
    "- All known protein interactions for each SI (ppi, kinase-substrate, etc)\n",
    "- Many of these interactions are directed (kinase-substrate, metabolic pathway, etc). PPI are not a directed interaction.\n",
    "\n",
    "Input: plain csv text file\n",
    "\n",
    "Csv format:\n",
    "SI_submodule,Shared_Interactor,SI_name,Motif_Containing_Proteins,submodule_Name\n",
    ",Interaction_Directionality\n",
    "\n",
    "YLR164C_Repressed_..RR.s.No_Phenotype_Exists,YLR164C,Tpk1,YDR207C,\n",
    "Repressed_..RR.s.No_Phenotype_Exists, kinase_substrate\n",
    "\n",
    "Column order is unimportant, column names must match above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yeast_Gene_name_to_ORF as yg          # used to get standard name\n",
    "\n",
    "# create input files \n",
    "# SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv - input from ID shared interactors\n",
    "submod = dict()\n",
    "with open('SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        row = line.rstrip().split(',')\n",
    "        submod[row[0] + '_' + row[2]] = row\n",
    "\n",
    "f.close()\n",
    "\n",
    "# get network information\n",
    "with open('classify_sharedInteractors_input.csv', 'w') as out:\n",
    "    header = '%s,%s,%s,%s,%s,%s\\n' %('SI_submodule','Shared_Interactor','SI_name','Motif_Containing_Proteins','submodule_Name'\n",
    ",'Interaction_Directionality')\n",
    "    out.write(header)\n",
    "    with open('Final_enriched.csv') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('M'):\n",
    "                continue\n",
    "            row = line.rstrip().split(',')\n",
    "            if int(row[12]) != 1:\n",
    "                continue\n",
    "            name = row[1] + '_' + row[3]\n",
    "            if name in submod:\n",
    "                if row[1].endswith(('A','B')):\n",
    "                    tmp = list(row[1])\n",
    "                    tmp.insert(-1,'-')\n",
    "                    row[1] = \"\".join(tmp)\n",
    "                n = re.sub('-', '', row[1])\n",
    "                ln = name + ',' + n + ',' + yg.sc_orfToGene[row[1]] + ',' + row[-1] + ',' + row[3] + ',' + row[2] + '\\n'\n",
    "                out.write(ln)\n",
    "\n",
    "Input_df=pd.read_csv('classify_sharedInteractors_input.csv')\n",
    "\n",
    "def Split_based_on_SI_submodule_Column():\n",
    "    ''' Function splits the input DF into independent DFs based on the SI-submodule column pairs. Thus, each SI and it's submodule protein interactions are\n",
    "    in independent dataframes '''\n",
    "    DF_lst =[]\n",
    "    for SI_submodule in Input_df['SI_submodule'].unique():\n",
    "        DF=Input_df.loc[Input_df['SI_submodule']==SI_submodule]\n",
    "        DF_lst.append(DF)\n",
    "    return DF_lst\n",
    "\n",
    "DF_lst=Split_based_on_SI_submodule_Column()\n",
    "\n",
    "def Count_Instances_of_Reverse_Interaction():\n",
    "    ''' Function counts, for each DF, and thus each SI-submodule pair, how many of the interactions are 'reversed', or facing from submodule TOWARDS SI. \n",
    "        It also counts the length of the dataframe, and then subtracts the the length of the dataframe from the counts. If the resultant value is 0, then all of the interactions \n",
    "        were reversed '''\n",
    "    DF_Counts_lst=[]\n",
    "    for df in DF_lst:\n",
    "        df=df.copy()\n",
    "        df['Counts']=df.Interaction_Directionality.str.contains('Reversed').sum()                                                               # Count the number of interactions that are \"Reversed\"\n",
    "        x=len(df)\n",
    "        df['Length']=x\n",
    "        df['Counts_Length']=df['Counts']-df['Length']\n",
    "        \n",
    "        DF_Counts_lst.append(df)\n",
    "    return DF_Counts_lst\n",
    "\n",
    "DF_Counts_lst=Count_Instances_of_Reverse_Interaction()\n",
    "\n",
    "def Only_Reverse_Interactions_Move_to_Outgoing_Columns():\n",
    "    '''Function assigns 'Input' and 'Output' classifications based on the 'Counts_Length' column in the dataframe. '0' values are 'outputs', all other's are 'inputs' '''\n",
    "    df_Modified_Outgoing_lst=[]\n",
    "    for df in DF_Counts_lst:\n",
    "        for value in df['Counts_Length'].unique():\n",
    "           \n",
    "            if value == 0:\n",
    "                df['Shared_Interactor_submodule_Relationship']= 'Output'\n",
    "                df_Modified_Outgoing_lst.append(df)\n",
    "            else:\n",
    "                df['Shared_Interactor_submodule_Relationship']= 'Input'\n",
    "                df_Modified_Outgoing_lst.append(df)\n",
    "           \n",
    "    return df_Modified_Outgoing_lst\n",
    "            \n",
    "df_Modified_Outgoing_lst=Only_Reverse_Interactions_Move_to_Outgoing_Columns()\n",
    "\n",
    "\n",
    "def AppendDFs(): \n",
    "    '''Function appends all dataframes back together '''\n",
    "    EmptyDF = pd.DataFrame()\n",
    "    for df in df_Modified_Outgoing_lst: \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df) \n",
    "    return EmptyDF\n",
    "\n",
    "Final=AppendDFs()    \n",
    "\n",
    "Final_Keep_Columns_Needed_For_SIF=Final[['SI_submodule', 'Shared_Interactor', 'submodule_Name', 'Shared_Interactor_submodule_Relationship']]  \n",
    "Final_Keep_Columns_Needed_For_SIF=Final_Keep_Columns_Needed_For_SIF.drop_duplicates('SI_submodule')                                                                     # Dropping duplicates entries, which are created because for each SI-submodule interaction there are numerous interactions with protein constituent. Only want a single interaction, input or output, for each SI and it's submodule. \n",
    "\n",
    "# create a new dataframe and write results to file\n",
    "myDF = pd.DataFrame(Final_Keep_Columns_Needed_For_SIF)\n",
    "filename = 'SIs_submodule_Relationships_Define_ClassA_Network.csv'\n",
    "myDF.to_csv(filename, index=False, encoding='utf-8',sep='\\t') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fasta file\n",
    "\n",
    "Input is file from Identify_Modules_and_Submodules step, 'Modules_pPep.csv'.  This file is parsed to produce\n",
    "\n",
    "All peptide sequences should be the same length (13 amino acids).\n",
    "\n",
    "Module constituents should be used here, not submodules. \n",
    "Fasta Files for each module will be created in a dir called: FastaFiles_Modules/\n",
    "\n",
    "The output Fasta format files are named with their module designation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open & parse file\n",
    "with open('pwm_input.csv', 'w') as out:\n",
    "    header = 'Module,Name,Sequence\\n'\n",
    "    out.write(header)\n",
    "    with open('Modules_pPep.csv','r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('\\tPpep'):\n",
    "                continue\n",
    "            dat = line.split('\\t')\n",
    "            outrow = '%s_%s,%s,%s\\n' %(dat[2],dat[3], dat[1], dat[4])\n",
    "            out.write(outrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "Input_df=pd.read_csv('pwm_input.csv')\n",
    "\n",
    "def Split_Into_SeparateDFs():\n",
    "    ''' Function splits the input dataframe, based on the module name, into independent dataframes for each module'''\n",
    "    df_lst=[]\n",
    "    for Module in Input_df['Module'].unique():\n",
    "        DF=Input_df.loc[Input_df['Module']==Module]\n",
    "        df_lst.append(DF)\n",
    "        \n",
    "    return df_lst\n",
    "\n",
    "df_lst=Split_Into_SeparateDFs()\n",
    "#print (df_lst)\n",
    "\n",
    "if not os.path.exists('FastaFiles_Modules'):\n",
    "    os.mkdir('FastaFiles_Modules')\n",
    "\n",
    "def CreateIndividualFastaFiles():\n",
    "    '''Function creates individual fasta files for each module nd writes them out to a user defined directory'''\n",
    "    for df in df_lst:                \n",
    "        Module_lst=df[\"Module\"].tolist()    \n",
    "        for name in Module_lst:          \n",
    "        # open a new file that contains the module name. USER can Change directory here.\n",
    "            ofile= open(\"FastaFiles_Modules/\"+name+\".fasta\", \"w\") \n",
    "        \n",
    "            df_lstName=df['Name'].tolist()             # send the module names to a list\n",
    "            df_lstSeq=df['Sequence'].tolist()          # send the peptide sequences to a list \n",
    "            \n",
    "            for i in range(len(df_lstSeq)):                    \n",
    "                \n",
    "                ofile.write(\">\" + df_lstName[i] + \"\\n\" + df_lstSeq[i] + \"\\n\")                                            # create a fasta file where the peptide name will be followed by the peptide sequence, on a new line\n",
    "           \n",
    "        df_lstName=[]                                                                                                    # empty each of the lists for the next iteration\n",
    "        df_lstSeq=[]\n",
    "        Module_lst=[]\n",
    "        ofile.close           \n",
    "        \n",
    "    return \n",
    "      \n",
    "CreateIndividualFastaFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PWMs from Module Fasta\n",
    "\n",
    "Generate PWMs for each module, using the module Fasta files. Module PWMs\n",
    "can then be compared to PWMs for 63 known kinase recognition motifs (Mok et al.,\n",
    "2010).\n",
    "\n",
    "Input: A directory containing files in Fasta format.\n",
    "\n",
    "Script uses BioPython to generate position weight matrices from a directory containing Fasta files for\n",
    "each modules phospho-peptides. \n",
    "\n",
    "### Note: \n",
    "Duplicate amino acid sequences should be removed from the Fasta files before running this script, if they exist, to prevent overweighting the matrix\n",
    "\n",
    "\n",
    "    \n",
    "output file should look like:\n",
    "\n",
    "    Motif,AA,0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "    Induced_...R.NS......,A:,0.044444444444444446,0.044444444444444446,0.044444444444444446, etc...\n",
    "    Induced_...R.NS......,C:,0.022222222222222223,0.022222222222222223,0.022222222222222223, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove duplicate sequences from each fasta file\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "clean = dict()\n",
    "\n",
    "for fasta in glob.glob('FastaFiles_Modules/*'):\n",
    "    print('processing file: %s ' %(fasta))\n",
    "    for seq_record in SeqIO.parse(fasta, 'fasta'):   # create Seq objects\n",
    "        s = str(seq_record.seq)\n",
    "        if s not in clean:\n",
    "            clean[s] = seq_record                    # only keep unique sequences\n",
    "            \n",
    "    out_handle = open('tmp.fasta', 'w')              \n",
    "    \n",
    "    for k,v in clean.items():                        # write unique sequences to tmp file  \n",
    "        SeqIO.write(v, out_handle, 'fasta')\n",
    "    out_handle.close()\n",
    "            \n",
    "    shutil.move('tmp.fasta', fasta)                  # overwrite original fasta file    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Bio\n",
    "import os\n",
    "import re\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "from Bio.Alphabet import IUPAC\n",
    "\n",
    "alphabet = IUPAC.protein           # use protein alphabet\n",
    "instances = []\n",
    "# list of amino acids used to print the position weight matrix\n",
    "AminoList = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y' ]\n",
    "# column numbers for printing pwm, length of peptide, assumed to be 13, if different change last value\n",
    "pep_Header = ','.join([str(i) for i in range(0,13)])     \n",
    "\n",
    "# user defined directory containing Fasta files\n",
    "#os.chdir(\"/home/mplace/projects/forMatt/Phospho_Network/\")  \n",
    "\n",
    "def CreatePWM():\n",
    "    ''' Function creates PWMs for each Module '''\n",
    "    instances = []\n",
    "    with open('position_weight_matrix.txt', 'w') as out:\n",
    "        out.write('Motif,AA,%s\\n' %(pep_Header))                   \n",
    "        for x in os.listdir('FastaFiles_Modules/'):                 # Iterate through the Fasta files in the directory\n",
    "            if x.endswith('.fasta'):\n",
    "                with open('FastaFiles_Modules/' + x, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        if line.startswith('>'):                                       \n",
    "                            continue\n",
    "                        line = line.rstrip()                                                 \n",
    "                        instances.append(Seq(line, IUPAC.protein))  # add amino acid sequence to instances\n",
    "                    m = motifs.create(instances)\n",
    "                    pwm = m.counts.normalize(pseudocounts = 1)      # Add a +1 pseudocount\n",
    "                    instances = []\n",
    "                    name = re.sub('.fasta', '', x)                  # use file name for 1st column          \n",
    "                    for aa in AminoList :\n",
    "                        score = [ str(i) for i in pwm[aa]]\n",
    "                        score = ','.join(score)\n",
    "                        out.write('%s,%s:,%s\\n' %(name,aa,score))\n",
    "    out.close()\n",
    "                    \n",
    "CreatePWM()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Kullback-Leibler Module to Each Kinase\n",
    "\n",
    "Purpose:  \n",
    "\n",
    "To quantify similarity between the Mok et. al. kinase PWMs and the module\n",
    "PWMs. Script employs a previously described quantitative motif comparison method\n",
    "called Kullback-Leibler divergence (KLD) (Thijs et al., 2002, Gupta et al., 2007).\n",
    "KLD generates a similarity measure by comparing the Kullback-Leiber distance, or\n",
    "information content, for each amino acid at each position between a query and\n",
    "comparison PWM. The more alike two PWMs are, the closer to zero the score approaches.\n",
    "\n",
    "    KLD(X,Y) = 1/2 (E Xalog(Xa/Ya) + E Yalog(Ya/Xa))\n",
    "\n",
    "Where X represents a query PWM position and Y a comparison PWM position.\n",
    "Xa indicates the probability of a given amino acid a  A in X. \n",
    "The symbol A represents the length of the motif alphabet, which is 20, \n",
    "representing each of the naturally occurring amino acids. \n",
    "\n",
    "\n",
    "\n",
    "Input:\n",
    "A plain text .csv file that contains all module position weight matrices. Each\n",
    "module PWM should have 20 rows, representing each of the 20 naturally occurring\n",
    "amino acids. They are in a column called \"AA\" which stands for amino acid. There\n",
    "should also be 13 columns, labeled 0-12 (representing the 13 amino acid sequence length\n",
    "of the phospho-peptides used to build the position weight matrix) that contain the\n",
    "frequency of each amino acid at each position.\n",
    "\n",
    "Csv file format\n",
    "Motif,AA,0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "Induced_...sP.,P:,0.05,0.05,0.03, 0.05,0.05,0.03,0.05,0.05,0.03, 0.05,0.05,0.03\n",
    "\n",
    "In addition, a directory that contains the Mok et al kinase PWMs. They have the identical\n",
    "format as above. They have been pre-generated and are available for download on Github.\n",
    "The repository is titled, \"Mok_kinase_PWMs\"\n",
    "\n",
    "Required Parameters: Pandas must be installed on your machine.\n",
    "\n",
    "Output: A directory containing plain text .csv files named after each module (ie.\n",
    "Induced_...sP..txt). Within the .csv files are 63 KLD scores representing how well the\n",
    "63 Mok et al kinases match the module motif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "\n",
    "Compare_To=pd.read_csv('position_weight_matrix.txt')   # input module pwm                                                                                    # The PWMs for the Modules.\n",
    "\n",
    "def DF_to_TSV(dataframe, NewFileName): \n",
    "    ''' Function writes out dataframes as TSV files'''\n",
    "    #path ='' \n",
    "    dataframe.to_csv (NewFileName,sep='\\t')  \n",
    "\n",
    "def SplitCompareTOMotifs_df():\n",
    "    ''' Function splits the Compare_To DF by Motif, which is listed in the \"Motif\" column, \n",
    "        and puts the new dataframes into a list\n",
    "    '''\n",
    "    DF_CompareTo_lst =[]\n",
    "    for Motif in Compare_To['Motif'].unique():\n",
    "        DF=Compare_To.loc[Compare_To['Motif']==Motif]\n",
    "        DF_CompareTo_lst.append(DF)\n",
    "    return DF_CompareTo_lst\n",
    "\n",
    "DF_CompareTo_lst=SplitCompareTOMotifs_df()\n",
    "\n",
    "def SplitInput_df_byMotif():\n",
    "    ''' Split the Input dataframe by Motif and create indpendent dataframes'''\n",
    "    DF_Input_lst =[]\n",
    "    for Motif in Input['Motif'].unique():\n",
    "        DF=Input.loc[Input['Motif']==Motif]\n",
    "        DF_Input_lst.append(DF)\n",
    "    return DF_Input_lst\n",
    "\n",
    "def Copy(df):\n",
    "    ''' Function makes a copy of a dataframe.  '''\n",
    "    df=df.copy()\n",
    "    return df\n",
    "\n",
    "def mergeInputMotifFile_withDF_CompareTo(df_Input,df_CompareTo):\n",
    "    ''' Merge the query and comparison PWMs so that KLD can be calculated by comparing column values'''\n",
    "    df_merged=df_Input.merge(df_CompareTo, on='AA')\n",
    "  \n",
    "    return df_merged\n",
    "\n",
    "def Calculate_log_x_y(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the query/comparison motifs'''\n",
    "        df['0_log(x/y)'] = df.apply(lambda x: math.log(x['0_x'],2) - math.log(x['0_y'],2), axis=1)\n",
    "        df['1_log(x/y)'] = df.apply(lambda x: math.log(x['1_x'],2) - math.log(x['1_y'],2), axis=1)\n",
    "        df['2_log(x/y)'] = df.apply(lambda x: math.log(x['2_x'],2) - math.log(x['2_y'],2), axis=1)\n",
    "        df['3_log(x/y)'] = df.apply(lambda x: math.log(x['3_x'],2) - math.log(x['3_y'],2), axis=1)\n",
    "        df['4_log(x/y)'] = df.apply(lambda x: math.log(x['4_x'],2) - math.log(x['4_y'],2), axis=1)\n",
    "        df['5_log(x/y)'] = df.apply(lambda x: math.log(x['5_x'],2) - math.log(x['5_y'],2), axis=1)\n",
    "        df['6_log(x/y)'] = df.apply(lambda x: math.log(x['6_x'],2) - math.log(x['6_y'],2), axis=1)\n",
    "        df['7_log(x/y)'] = df.apply(lambda x: math.log(x['7_x'],2) - math.log(x['7_y'],2), axis=1)\n",
    "        df['8_log(x/y)'] = df.apply(lambda x: math.log(x['8_x'],2) - math.log(x['8_y'],2), axis=1)\n",
    "        df['9_log(x/y)'] = df.apply(lambda x: math.log(x['9_x'],2) - math.log(x['9_y'],2), axis=1)\n",
    "        df['10_log(x/y)'] = df.apply(lambda x: math.log(x['10_x'],2) - math.log(x['10_y'],2), axis=1)\n",
    "        df['11_log(x/y)'] = df.apply(lambda x: math.log(x['11_x'],2) - math.log(x['11_y'],2), axis=1)\n",
    "        df['12_log(x/y)'] = df.apply(lambda x: math.log(x['12_x'],2) - math.log(x['12_y'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "def Calculate_log_y_x(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the comparison/query motifs'''\n",
    "        df['0_log(y/x)'] = df.apply(lambda x: math.log(x['0_y'],2) - math.log(x['0_x'],2), axis=1)\n",
    "        df['1_log(y/x)'] = df.apply(lambda x: math.log(x['1_y'],2) - math.log(x['1_x'],2), axis=1)\n",
    "        df['2_log(y/x)'] = df.apply(lambda x: math.log(x['2_y'],2) - math.log(x['2_x'],2), axis=1)\n",
    "        df['3_log(y/x)'] = df.apply(lambda x: math.log(x['3_y'],2) - math.log(x['3_x'],2), axis=1)\n",
    "        df['4_log(y/x)'] = df.apply(lambda x: math.log(x['4_y'],2) - math.log(x['4_x'],2), axis=1)\n",
    "        df['5_log(y/x)'] = df.apply(lambda x: math.log(x['5_y'],2) - math.log(x['5_x'],2), axis=1)\n",
    "        df['6_log(y/x)'] = df.apply(lambda x: math.log(x['6_y'],2) - math.log(x['6_x'],2), axis=1)\n",
    "        df['7_log(y/x)'] = df.apply(lambda x: math.log(x['7_y'],2) - math.log(x['7_x'],2), axis=1)\n",
    "        df['8_log(y/x)'] = df.apply(lambda x: math.log(x['8_y'],2) - math.log(x['8_x'],2), axis=1)\n",
    "        df['9_log(y/x)'] = df.apply(lambda x: math.log(x['9_y'],2) - math.log(x['9_x'],2), axis=1)\n",
    "        df['10_log(y/x)'] = df.apply(lambda x: math.log(x['10_y'],2) - math.log(x['10_x'],2), axis=1)\n",
    "        df['11_log(y/x)'] = df.apply(lambda x: math.log(x['11_y'],2) - math.log(x['11_x'],2), axis=1)\n",
    "        df['12_log(y/x)'] = df.apply(lambda x: math.log(x['12_y'],2) - math.log(x['12_x'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "def Calculate_Faax_times_log_x_y(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faax) \"Xa\" at a specific position in the query motif against the log(Xa/Ya) for that amino acid\n",
    "     It is calculating this part of the function  \"Xalog(Xa/Ya)\" '''\n",
    "\n",
    "    df['F(aax)*0_log(x/y)']=df['0_x']*df['0_log(x/y)']\n",
    "    df['F(aax)*1_log(x/y)']=df['1_x']*df['1_log(x/y)']\n",
    "    df['F(aax)*2_log(x/y)']=df['2_x']*df['2_log(x/y)']\n",
    "    df['F(aax)*3_log(x/y)']=df['3_x']*df['3_log(x/y)']\n",
    "    df['F(aax)*4_log(x/y)']=df['4_x']*df['4_log(x/y)']\n",
    "    df['F(aax)*5_log(x/y)']=df['5_x']*df['5_log(x/y)']\n",
    "    df['F(aax)*6_log(x/y)']=df['6_x']*df['6_log(x/y)']\n",
    "    df['F(aax)*7_log(x/y)']=df['7_x']*df['7_log(x/y)']\n",
    "    df['F(aax)*8_log(x/y)']=df['8_x']*df['8_log(x/y)']\n",
    "    df['F(aax)*9_log(x/y)']=df['9_x']*df['9_log(x/y)']\n",
    "    df['F(aax)*10_log(x/y)']=df['10_x']*df['10_log(x/y)']\n",
    "    df['F(aax)*11_log(x/y)']=df['11_x']*df['11_log(x/y)']\n",
    "    df['F(aax)*12_log(x/y)']=df['12_x']*df['12_log(x/y)']\n",
    "    return df\n",
    "\n",
    "def Calculate_Faay_times_log_y_x(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faay) \"Ya\" at a specific position in the query motif against the log(Ya/Xa) for that amino acid\n",
    "     It is calculating this part of the function  \"Yalog(Ya/Xa)\" '''\n",
    "    df['F(aay)*0_log(y/x)']=df['0_y']*df['0_log(y/x)']\n",
    "    df['F(aay)*1_log(y/x)']=df['1_y']*df['1_log(y/x)']\n",
    "    df['F(aay)*2_log(y/x)']=df['2_y']*df['2_log(y/x)']\n",
    "    df['F(aay)*3_log(y/x)']=df['3_y']*df['3_log(y/x)']\n",
    "    df['F(aay)*4_log(y/x)']=df['4_y']*df['4_log(y/x)']\n",
    "    df['F(aay)*5_log(y/x)']=df['5_y']*df['5_log(y/x)']\n",
    "    df['F(aay)*6_log(y/x)']=df['6_y']*df['6_log(y/x)']\n",
    "    df['F(aay)*7_log(y/x)']=df['7_y']*df['7_log(y/x)']\n",
    "    df['F(aay)*8_log(y/x)']=df['8_y']*df['8_log(y/x)']\n",
    "    df['F(aay)*9_log(y/x)']=df['9_y']*df['9_log(y/x)']\n",
    "    df['F(aay)*10_log(y/x)']=df['10_y']*df['10_log(y/x)']\n",
    "    df['F(aay)*11_log(y/x)']=df['11_y']*df['11_log(y/x)']\n",
    "    df['F(aay)*12_log(y/x)']=df['12_y']*df['12_log(y/x)']\n",
    "    return df\n",
    "\n",
    "def Column_SUM(df):\n",
    "    ''' Function sums the values calculated by the previous two functions for each position, or column, in the PWMs  '''\n",
    "    df['sum_0']=sum(df['F(aax)*0_log(x/y)'])+sum(df['F(aay)*0_log(y/x)'])\n",
    "    df['sum_1']=sum(df['F(aax)*1_log(x/y)'])+sum(df['F(aay)*1_log(y/x)'])\n",
    "    df['sum_2']=sum(df['F(aax)*2_log(x/y)'])+sum(df['F(aay)*2_log(y/x)'])\n",
    "    df['sum_3']=sum(df['F(aax)*3_log(x/y)'])+sum(df['F(aay)*3_log(y/x)'])\n",
    "    df['sum_4']=sum(df['F(aax)*4_log(x/y)'])+sum(df['F(aay)*4_log(y/x)'])\n",
    "    df['sum_5']=sum(df['F(aax)*5_log(x/y)'])+sum(df['F(aay)*5_log(y/x)'])\n",
    "    df['sum_6']=sum(df['F(aax)*6_log(x/y)'])+sum(df['F(aay)*6_log(y/x)'])\n",
    "    df['sum_7']=sum(df['F(aax)*7_log(x/y)'])+sum(df['F(aay)*7_log(y/x)'])\n",
    "    df['sum_8']=sum(df['F(aax)*8_log(x/y)'])+sum(df['F(aay)*8_log(y/x)'])\n",
    "    df['sum_9']=sum(df['F(aax)*9_log(x/y)'])+sum(df['F(aay)*9_log(y/x)'])\n",
    "    df['sum_10']=sum(df['F(aax)*10_log(x/y)'])+sum(df['F(aay)*10_log(y/x)'])\n",
    "    df['sum_11']=sum(df['F(aax)*11_log(x/y)'])+sum(df['F(aay)*11_log(y/x)'])\n",
    "    df['sum_12']=(sum(df['F(aax)*12_log(x/y)'])+sum(df['F(aay)*12_log(y/x)']))\n",
    "    return df\n",
    "\n",
    "def TotalScore(df):\n",
    "    ''' Function calculates the total score by summing the summed values for each position in the PWM (13 positions)'''\n",
    "    df['FinalScore']=df['sum_0']+df['sum_1']+df['sum_2']+df['sum_3']+df['sum_4']+df['sum_5']+df['sum_6']+df['sum_7']+df['sum_8']+df['sum_9']+df['sum_10']+df['sum_11']+df['sum_12']\n",
    "    Lst=df['FinalScore'].unique()\n",
    "    n=Lst[0]\n",
    "    return n\n",
    "\n",
    "# Import the Mok Kinases PWM .csv files individually and create dataframes\n",
    "path=r\"Mok_kinase_PWMs/\"\n",
    "filenames = glob.glob(path + \"*.csv\")\n",
    "\n",
    "dfs_lst = []\n",
    "for filename in filenames:\n",
    "    dfs_lst.append(pd.read_csv(filename, sep=\",\"))\n",
    "    \n",
    "ITER_NUM=1                                          # One iteration of the below function. \n",
    "dict_Final={}\n",
    "for df2 in dfs_lst:                                 # This is the dataframe that has Module PWMs\n",
    "    \n",
    "    subModule_name=df2['Motif'].unique()\n",
    "    for df in DF_CompareTo_lst:                     # select one of the compare to dataframes (Mok Kinase PWM)\n",
    "        Kinase_name=[]\n",
    "        Kinase_name=df['Motif'].unique()\n",
    " \n",
    "        for iteration in range (ITER_NUM):                                      # for the first iteration \n",
    "\n",
    "            Copied=Copy(df2)                                                    # Copy Dataframe\n",
    "            # Create a merged version of the dataframe for each Kinase PWM and each Module PWM\n",
    "            df_merged=mergeInputMotifFile_withDF_CompareTo(Copied, df)  \n",
    "\n",
    "            DF_1=Calculate_log_x_y(df_merged)                                   \n",
    "            DF_2=Calculate_log_y_x(DF_1)\n",
    "            DF_3=Calculate_Faax_times_log_x_y(DF_2)\n",
    "            DF_4=Calculate_Faay_times_log_y_x(DF_3)\n",
    "            DF_5=Column_SUM(DF_4)\n",
    "        \n",
    "            n=TotalScore(DF_5)                                                  \n",
    "            #print (n)\n",
    "            test_tup = (n, subModule_name[0])\n",
    "            if Kinase_name[0] in dict_Final:\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "            else:\n",
    "                lst=[]\n",
    "                dict_Final[Kinase_name[0]] = lst\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "\n",
    "\n",
    "# write out the final dictionary to a folder where each key and value pair is an independent csv file. \n",
    "if not os.path.exists('ClassA_NoShuffle_KL'):\n",
    "    os.mkdir('ClassA_NoShuffle_KL')\n",
    "    \n",
    "path=r\"ClassA_NoShuffle_KL/\"              # this is the path to the folder where the output files will be housed\n",
    "\n",
    "for k, v in dict_Final.items():           # select each key and value pair in the dict \n",
    "    newFile=path+ k +'.csv'               # create newFile, that will have the path and name (the key, which is the kinase) associated with it\n",
    "    #print (newFile)\n",
    "    with open(newFile, 'w') as output:  \n",
    "        output.write(k)\n",
    "        output.write(\"\\n\")\n",
    "        for x in v:\n",
    "           \n",
    "            output.write(str(x))\n",
    "            output.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler Module to Each Kinase Shuffled 1000x\n",
    "\n",
    "\n",
    "Purpose:  The same algorithm as the last step is used but w/ 1000 Shuffles of the Mok Kinase PWMs are performed by the script, generating randomized PWMs that are compared against the Module PWMs, producing a distribution of scores.\n",
    "\n",
    "Output: A directory containing plain text .csv files named after each module. Within\n",
    "the .csv files are 63,000 KLD scores representing how well the 63 Mok et al kinases\n",
    "match the module motif after 1000 permutations of each Mok kinase.\n",
    "\n",
    "# ITERATIONS CURRENTLY SET TO 5 FOR TESTING  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import os\n",
    " \n",
    "Compare_To=pd.read_csv('position_weight_matrix.txt')\n",
    "\n",
    "\n",
    "def SplitCompareTOMotifs_df():\n",
    "    ''' Function splits the Compare_To DF by Motif, which is listed in the \"Motif\" column, and puts the new dataframes into a list'''\n",
    "    DF_CompareTo_lst =[]\n",
    "    for Motif in Compare_To['Motif'].unique():\n",
    "        DF=Compare_To.loc[Compare_To['Motif']==Motif]\n",
    "        DF_CompareTo_lst.append(DF)\n",
    "    return DF_CompareTo_lst\n",
    "\n",
    "DF_CompareTo_lst=SplitCompareTOMotifs_df()\n",
    "\n",
    "\n",
    "def SplitInput_df_byMotif():\n",
    "    ''' Split the Input dataframe by Motif and create indpendent dataframes'''\n",
    "    DF_Input_lst =[]\n",
    "    for Motif in Input['Motif'].unique():\n",
    "        DF=Input.loc[Input['Motif']==Motif]\n",
    "        DF_Input_lst.append(DF)\n",
    "    return DF_Input_lst\n",
    "\n",
    "#DF_Input_lst=SplitInput_df_byMotif()\n",
    "#Unique_Input_Motif_Names_lst=Input['Motif'].unique()\n",
    "\n",
    "def Shuffle(df):\n",
    "    '''Shuffle each column by row, after first creating independent dataframes for each column (position within the PWM)''' \n",
    "\n",
    "    df_Index = df[['Motif','AA']]  \n",
    "    df_data_0 = df['0']#,'1','2','3','4','5','7','8','9','10','11','12']]\n",
    "    df_data_1 = df['1']#'2','3','4','5','7','8','9','10','11','12']]\n",
    "    df_data_2 = df['2']\n",
    "    df_data_3 = df['3']\n",
    "    df_data_4 = df['4']\n",
    "    df_data_5 = df['5']\n",
    "    df_data_6 = df['6']\n",
    "    df_data_7 = df['7']\n",
    "    df_data_8 = df['8']\n",
    "    df_data_9 = df['9']\n",
    "    df_data_10 = df['10']\n",
    "    df_data_11 = df['11']\n",
    "    df_data_12 = df['12']\n",
    "    Shuffled_Input_0=df_data_0.iloc[np.random.permutation(len(df_data_0))] \n",
    "    Shuffled_Input_1=df_data_1.iloc[np.random.permutation(len(df_data_1))]                                  # Shuffle the data by row\n",
    "    Shuffled_Input_2=df_data_2.iloc[np.random.permutation(len(df_data_2))] \n",
    "    Shuffled_Input_3=df_data_3.iloc[np.random.permutation(len(df_data_3))] \n",
    "    Shuffled_Input_4=df_data_4.iloc[np.random.permutation(len(df_data_4))] \n",
    "    Shuffled_Input_5=df_data_5.iloc[np.random.permutation(len(df_data_5))] \n",
    "    Shuffled_Input_6=df_data_6.iloc[np.random.permutation(len(df_data_6))] \n",
    "    Shuffled_Input_7=df_data_7.iloc[np.random.permutation(len(df_data_7))] \n",
    "    Shuffled_Input_8=df_data_8.iloc[np.random.permutation(len(df_data_8))]\n",
    "    Shuffled_Input_9=df_data_9.iloc[np.random.permutation(len(df_data_9))] \n",
    "    Shuffled_Input_10=df_data_10.iloc[np.random.permutation(len(df_data_10))] \n",
    "    Shuffled_Input_11=df_data_11.iloc[np.random.permutation(len(df_data_11))] \n",
    "    Shuffled_Input_12=df_data_12.iloc[np.random.permutation(len(df_data_12))] \n",
    "\n",
    "    Shuffled_Input_0_reset_Index= Shuffled_Input_0.reset_index(drop=True)                                   # reset the index for a later merge\n",
    "    Shuffled_Input_1_reset_Index= Shuffled_Input_1.reset_index(drop=True)\n",
    "    Shuffled_Input_2_reset_Index= Shuffled_Input_2.reset_index(drop=True) \n",
    "    Shuffled_Input_3_reset_Index= Shuffled_Input_3.reset_index(drop=True)\n",
    "    Shuffled_Input_4_reset_Index= Shuffled_Input_4.reset_index(drop=True) \n",
    "    Shuffled_Input_5_reset_Index= Shuffled_Input_5.reset_index(drop=True)\n",
    "    Shuffled_Input_6_reset_Index= Shuffled_Input_6.reset_index(drop=True) \n",
    "    Shuffled_Input_7_reset_Index= Shuffled_Input_7.reset_index(drop=True)\n",
    "    Shuffled_Input_8_reset_Index= Shuffled_Input_8.reset_index(drop=True) \n",
    "    Shuffled_Input_9_reset_Index= Shuffled_Input_9.reset_index(drop=True)\n",
    "    Shuffled_Input_10_reset_Index= Shuffled_Input_10.reset_index(drop=True)\n",
    "    Shuffled_Input_11_reset_Index= Shuffled_Input_11.reset_index(drop=True) \n",
    "    Shuffled_Input_12_reset_Index= Shuffled_Input_12.reset_index(drop=True)\n",
    "    \n",
    "    result = pd.concat([df_Index, Shuffled_Input_0_reset_Index, Shuffled_Input_1_reset_Index, Shuffled_Input_2_reset_Index, Shuffled_Input_3_reset_Index,\n",
    "                       Shuffled_Input_4_reset_Index, Shuffled_Input_5_reset_Index, Shuffled_Input_6_reset_Index, Shuffled_Input_7_reset_Index,\n",
    "                       Shuffled_Input_8_reset_Index, Shuffled_Input_9_reset_Index, Shuffled_Input_10_reset_Index, Shuffled_Input_11_reset_Index, Shuffled_Input_12_reset_Index], axis=1) # concatenate the dataframes - they will sit side by side since have the same index (numbering)\n",
    "    result_reordered=result[[ 'Motif', 'AA','0','1', '2','3','4','5','6','7','8','9','10','11','12']]       # reorder the columns so in the correct PWM order.\n",
    "    \n",
    "    result_reordered_Index=result_reordered[['Motif','AA']]\n",
    "    result_reordered_Frame=result_reordered[['0','1', '2','3','4','5','6','7','8','9','10','11','12']]\n",
    "    cols = result_reordered_Frame.columns.tolist()                                                          # send column headers to a list\n",
    "   \n",
    "    random.shuffle(cols)                                                                                    # shuffle the columns, which are in a list by name, and return a different order\n",
    " \n",
    "    \n",
    "    \n",
    "    FinalDF=result_reordered_Frame[cols]                                                                    # make a new dataframe with randomly shuffled columns\n",
    "    FinalDF.columns = ['0','1', '2','3','4','5','6','7','8','9','10','11','12']                             # reset the column names, so that they have the original names\n",
    "    FinalDataframe= pd.concat([result_reordered_Index, FinalDF],axis=1)\n",
    "    return FinalDataframe\n",
    "\n",
    "Shuffled=Shuffle(Input)\n",
    "\n",
    "def mergeInputMotifFile_withDF_CompareTo(df_Input,df_CompareTo):\n",
    "    ''' Merge the query and comparison PWMs so that KLD can be calculated by comparing column values'''\n",
    "    df_merged=df_Input.merge(df_CompareTo, on='AA')\n",
    "    #df_lst.append(df_merged)\n",
    "    return df_merged\n",
    "   \n",
    "df_merged=mergeInputMotifFile_withDF_CompareTo(Shuffled, Compare_To)\n",
    "\n",
    "def Calculate_log_x_y(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the query/comparison motifs'''\n",
    "        df['0_log(x/y)'] = df.apply(lambda x: math.log(x['0_x'],2) - math.log(x['0_y'],2), axis=1)\n",
    "        df['1_log(x/y)'] = df.apply(lambda x: math.log(x['1_x'],2) - math.log(x['1_y'],2), axis=1)\n",
    "        df['2_log(x/y)'] = df.apply(lambda x: math.log(x['2_x'],2) - math.log(x['2_y'],2), axis=1)\n",
    "        df['3_log(x/y)'] = df.apply(lambda x: math.log(x['3_x'],2) - math.log(x['3_y'],2), axis=1)\n",
    "        df['4_log(x/y)'] = df.apply(lambda x: math.log(x['4_x'],2) - math.log(x['4_y'],2), axis=1)\n",
    "        df['5_log(x/y)'] = df.apply(lambda x: math.log(x['5_x'],2) - math.log(x['5_y'],2), axis=1)\n",
    "        df['6_log(x/y)'] = df.apply(lambda x: math.log(x['6_x'],2) - math.log(x['6_y'],2), axis=1)\n",
    "        df['7_log(x/y)'] = df.apply(lambda x: math.log(x['7_x'],2) - math.log(x['7_y'],2), axis=1)\n",
    "        df['8_log(x/y)'] = df.apply(lambda x: math.log(x['8_x'],2) - math.log(x['8_y'],2), axis=1)\n",
    "        df['9_log(x/y)'] = df.apply(lambda x: math.log(x['9_x'],2) - math.log(x['9_y'],2), axis=1)\n",
    "        df['10_log(x/y)'] = df.apply(lambda x: math.log(x['10_x'],2) - math.log(x['10_y'],2), axis=1)\n",
    "        df['11_log(x/y)'] = df.apply(lambda x: math.log(x['11_x'],2) - math.log(x['11_y'],2), axis=1)\n",
    "        df['12_log(x/y)'] = df.apply(lambda x: math.log(x['12_x'],2) - math.log(x['12_y'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "DF_1=Calculate_log_x_y(df_merged)\n",
    "\n",
    "\n",
    "def Calculate_log_y_x(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the comparison/query motifs'''\n",
    "        df['0_log(y/x)'] = df.apply(lambda x: math.log(x['0_y'],2) - math.log(x['0_x'],2), axis=1)\n",
    "        df['1_log(y/x)'] = df.apply(lambda x: math.log(x['1_y'],2) - math.log(x['1_x'],2), axis=1)\n",
    "        df['2_log(y/x)'] = df.apply(lambda x: math.log(x['2_y'],2) - math.log(x['2_x'],2), axis=1)\n",
    "        df['3_log(y/x)'] = df.apply(lambda x: math.log(x['3_y'],2) - math.log(x['3_x'],2), axis=1)\n",
    "        df['4_log(y/x)'] = df.apply(lambda x: math.log(x['4_y'],2) - math.log(x['4_x'],2), axis=1)\n",
    "        df['5_log(y/x)'] = df.apply(lambda x: math.log(x['5_y'],2) - math.log(x['5_x'],2), axis=1)\n",
    "        df['6_log(y/x)'] = df.apply(lambda x: math.log(x['6_y'],2) - math.log(x['6_x'],2), axis=1)\n",
    "        df['7_log(y/x)'] = df.apply(lambda x: math.log(x['7_y'],2) - math.log(x['7_x'],2), axis=1)\n",
    "        df['8_log(y/x)'] = df.apply(lambda x: math.log(x['8_y'],2) - math.log(x['8_x'],2), axis=1)\n",
    "        df['9_log(y/x)'] = df.apply(lambda x: math.log(x['9_y'],2) - math.log(x['9_x'],2), axis=1)\n",
    "        df['10_log(y/x)'] = df.apply(lambda x: math.log(x['10_y'],2) - math.log(x['10_x'],2), axis=1)\n",
    "        df['11_log(y/x)'] = df.apply(lambda x: math.log(x['11_y'],2) - math.log(x['11_x'],2), axis=1)\n",
    "        df['12_log(y/x)'] = df.apply(lambda x: math.log(x['12_y'],2) - math.log(x['12_x'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "def Calculate_Faax_times_log_x_y(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faax) \"Xa\" at a specific position in the query motif against the log(Xa/Ya) for that amino acid\n",
    "     It is calculating this part of the function  \"Xalog(Xa/Ya)\" '''\n",
    "    df['F(aax)*0_log(x/y)']=df['0_x']*df['0_log(x/y)']\n",
    "    df['F(aax)*1_log(x/y)']=df['1_x']*df['1_log(x/y)']\n",
    "    df['F(aax)*2_log(x/y)']=df['2_x']*df['2_log(x/y)']\n",
    "    df['F(aax)*3_log(x/y)']=df['3_x']*df['3_log(x/y)']\n",
    "    df['F(aax)*4_log(x/y)']=df['4_x']*df['4_log(x/y)']\n",
    "    df['F(aax)*5_log(x/y)']=df['5_x']*df['5_log(x/y)']\n",
    "    df['F(aax)*6_log(x/y)']=df['6_x']*df['6_log(x/y)']\n",
    "    df['F(aax)*7_log(x/y)']=df['7_x']*df['7_log(x/y)']\n",
    "    df['F(aax)*8_log(x/y)']=df['8_x']*df['8_log(x/y)']\n",
    "    df['F(aax)*9_log(x/y)']=df['9_x']*df['9_log(x/y)']\n",
    "    df['F(aax)*10_log(x/y)']=df['10_x']*df['10_log(x/y)']\n",
    "    df['F(aax)*11_log(x/y)']=df['11_x']*df['11_log(x/y)']\n",
    "    df['F(aax)*12_log(x/y)']=df['12_x']*df['12_log(x/y)']\n",
    "    return df\n",
    "\n",
    "def Calculate_Faay_times_log_y_x(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faay) \"Ya\" at a specific position in the query motif against the log(Ya/Xa) for that amino acid\n",
    "     It is calculating this part of the function  \"Yalog(Ya/Xa)\" '''\n",
    "    df['F(aay)*0_log(y/x)']=df['0_y']*df['0_log(y/x)']\n",
    "    df['F(aay)*1_log(y/x)']=df['1_y']*df['1_log(y/x)']\n",
    "    df['F(aay)*2_log(y/x)']=df['2_y']*df['2_log(y/x)']\n",
    "    df['F(aay)*3_log(y/x)']=df['3_y']*df['3_log(y/x)']\n",
    "    df['F(aay)*4_log(y/x)']=df['4_y']*df['4_log(y/x)']\n",
    "    df['F(aay)*5_log(y/x)']=df['5_y']*df['5_log(y/x)']\n",
    "    df['F(aay)*6_log(y/x)']=df['6_y']*df['6_log(y/x)']\n",
    "    df['F(aay)*7_log(y/x)']=df['7_y']*df['7_log(y/x)']\n",
    "    df['F(aay)*8_log(y/x)']=df['8_y']*df['8_log(y/x)']\n",
    "    df['F(aay)*9_log(y/x)']=df['9_y']*df['9_log(y/x)']\n",
    "    df['F(aay)*10_log(y/x)']=df['10_y']*df['10_log(y/x)']\n",
    "    df['F(aay)*11_log(y/x)']=df['11_y']*df['11_log(y/x)']\n",
    "    df['F(aay)*12_log(y/x)']=df['12_y']*df['12_log(y/x)']\n",
    "    return df\n",
    "\n",
    "def Column_SUM(df):\n",
    "    ''' Function sums the values calculated by the previous two functions for each position, or column, in the PWMs  '''\n",
    "    df['sum_0']=sum(df['F(aax)*0_log(x/y)'])+sum(df['F(aay)*0_log(y/x)'])\n",
    "    df['sum_1']=sum(df['F(aax)*1_log(x/y)'])+sum(df['F(aay)*1_log(y/x)'])\n",
    "    df['sum_2']=sum(df['F(aax)*2_log(x/y)'])+sum(df['F(aay)*2_log(y/x)'])\n",
    "    df['sum_3']=sum(df['F(aax)*3_log(x/y)'])+sum(df['F(aay)*3_log(y/x)'])\n",
    "    df['sum_4']=sum(df['F(aax)*4_log(x/y)'])+sum(df['F(aay)*4_log(y/x)'])\n",
    "    df['sum_5']=sum(df['F(aax)*5_log(x/y)'])+sum(df['F(aay)*5_log(y/x)'])\n",
    "    df['sum_6']=sum(df['F(aax)*6_log(x/y)'])+sum(df['F(aay)*6_log(y/x)'])\n",
    "    df['sum_7']=sum(df['F(aax)*7_log(x/y)'])+sum(df['F(aay)*7_log(y/x)'])\n",
    "    df['sum_8']=sum(df['F(aax)*8_log(x/y)'])+sum(df['F(aay)*8_log(y/x)'])\n",
    "    df['sum_9']=sum(df['F(aax)*9_log(x/y)'])+sum(df['F(aay)*9_log(y/x)'])\n",
    "    df['sum_10']=sum(df['F(aax)*10_log(x/y)'])+sum(df['F(aay)*10_log(y/x)'])\n",
    "    df['sum_11']=sum(df['F(aax)*11_log(x/y)'])+sum(df['F(aay)*11_log(y/x)'])\n",
    "    df['sum_12']=(sum(df['F(aax)*12_log(x/y)'])+sum(df['F(aay)*12_log(y/x)']))\n",
    "    return df\n",
    "\n",
    "def TotalScore(df):\n",
    "    ''' Function calculates the total score by summing the summed values for each position in the PWM (13 positions)'''\n",
    "    df['FinalScore']=df['sum_0']+df['sum_1']+df['sum_2']+df['sum_3']+df['sum_4']+df['sum_5']+df['sum_6']+df['sum_7']+df['sum_8']+df['sum_9']+df['sum_10']+df['sum_11']+df['sum_12']\n",
    "    Lst=df['FinalScore'].unique()\n",
    "    n=Lst[0]\n",
    "    return n\n",
    "\n",
    "# Import the Mok Kinases PWM .csv files individually and create dataframes\n",
    "path=r\"Mok_kinase_PWMs/\"\n",
    "filenames = glob.glob(path + \"*.csv\")\n",
    "\n",
    "dfs_lst = []\n",
    "for filename in filenames:\n",
    "    dfs_lst.append(pd.read_csv(filename, sep=\",\"))\n",
    "    \n",
    "# CHANGE THE NUMBER OF ITERATIONS HERE IF DESIRED\n",
    "ITER_NUM=5                                                                          # 1000 interations of this function\n",
    "dict_Final={}\n",
    "for df2 in dfs_lst:                                                                 # this is the dataframe that has Mok Kinase PWMs\n",
    "    \n",
    "    subModule_name=df2['Motif'].unique()\n",
    "    for df in DF_CompareTo_lst:                                                     # select one of the compare to dataframes (Modules)\n",
    "        Kinase_name=[]\n",
    "        Kinase_name=df['Motif'].unique()\n",
    "   \n",
    "        for iteration in range (ITER_NUM):                                          # for iteration x \n",
    "    \n",
    "            Shuffled=Shuffle(df2)                                                   # shuffle the dataframe by row and column\n",
    "            df_merged=mergeInputMotifFile_withDF_CompareTo(Shuffled, df)   \n",
    "            DF_1=Calculate_log_x_y(df_merged)\n",
    "            DF_2=Calculate_log_y_x(DF_1)\n",
    "            DF_3=Calculate_Faax_times_log_x_y(DF_2)\n",
    "            DF_4=Calculate_Faay_times_log_y_x(DF_3)\n",
    "            DF_5=Column_SUM(DF_4)\n",
    "        \n",
    "            n=TotalScore(DF_5)\n",
    "            print (n)\n",
    "            test_tup = (n, subModule_name[0])\n",
    "            if Kinase_name[0] in dict_Final:\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "            else:\n",
    "                lst=[]\n",
    "                dict_Final[Kinase_name[0]] = lst\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "\n",
    "# write out the final dictionary to a folder where each key and value pair is an independent csv file. \n",
    "if not os.path.exists('Shuffle_KL'):\n",
    "    os.mkdir('Shuffle_KL')\n",
    "    \n",
    "path=r\"Shuffle_KL/\"  # path to output directory\n",
    "              \n",
    "for k, v in dict_Final.items():  \n",
    "    newFile=path+ k +'.csv'       \n",
    "    #print (newFile)\n",
    "    with open(newFile, 'w') as output:  \n",
    "        output.write(k)\n",
    "        output.write(\"\\n\")\n",
    "        for x in v:\n",
    "            #for subModule_name in filenames:\n",
    "            output.write(str(x))\n",
    "            output.write(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate FDR Each Module \n",
    "Purpose: Identify FDR scores for each Mok et. al. kinase and each module by comparing\n",
    "the non-shuffled scores to the distribution of shuffled scores. The user can then manually\n",
    "define the FDR cutoff to call kinases \"motif-match\" or \"non-match\" for a given module.\n",
    "\n",
    "Input: Two directories and a single plain text .csv file, called \"Kinases_Not_In_Mok.csv\"\n",
    "that is provided on the Github page. The first directory contains plain text .csv files with\n",
    "KLD scores for non-shuffled Mok et al kinases and Modules. The second directory\n",
    "contains plain text .csv files containing KLD scores for shuffled Mok et. al. kinases and\n",
    "Modules.\n",
    "\n",
    "Csv format (For both Input Directories)\n",
    "\n",
    "Scores,Kinase,Module,\n",
    "13.25,cdc15,Induced.sP.\n",
    "\n",
    "\n",
    "Output: A table that contains for each module, all yeast kinases, including those found in\n",
    "the Mok et al dataset and those that were absent, and their FDR scores for each module.\n",
    "Kinases not found in the Mok et al dataset are given an FDR score of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "''' This script is calculates an FDR for each Mok Kinase to each Module. \n",
    "The script takes the 63,000 shuffled Mok et al kinase-module scores and determines for each kinase where that unshuffled kinase-module score\n",
    "falls in the shuffled distribution.For example,if a Kinase has an shuffled score of 14.7 to a module , and only 63 unshuffled kinase-module scores\n",
    "are below that value, then this kinase has has an FDR of 0.1% (63/63,000 scores). We can then use the FDR values for all kinases to a module\n",
    "to determine an FDR cutoff for that module. Thus, we can say only these x kinases are a good match to the module. Calling an FDR threshold is \n",
    "done manually by the user.\n",
    "'''\n",
    "\n",
    "#Read in input files (which are the non-shuffled scores for all Mok kinases compared to each Module) \n",
    "path=r\"ClassA_NoShuffle_KL/\"\n",
    "filenames = glob.glob(path + \"*.csv\")\n",
    "\n",
    "labels = ['Scores','Kinase','Module']                # column header\n",
    "\n",
    "def load_data(filenames, dfs_input_lst):\n",
    "    ''' Read in files and parse to produce input data  '''\n",
    "    for i in filenames:\n",
    "        with open(i, 'r') as f:\n",
    "            mod_name = f.readline().rstrip()\n",
    "            for ln in f:\n",
    "                ln = ln.rstrip()\n",
    "                ln = re.sub('\\(', '', re.sub('\\)', '', re.sub('\\'','', re.sub('\\s', '', ln))))\n",
    "                row = (ln + ',' +mod_name).split(',')\n",
    "                dfs_input_lst.append(row)\n",
    "        f.close()\n",
    "    return dfs_input_lst\n",
    "\n",
    "dfs_input_lst = []\n",
    "Input = load_data(filenames, dfs_input_lst)\n",
    "Input = pd.DataFrame(dfs_input_lst, columns=labels)\n",
    "\n",
    "def SplitInput_df_byModule(data):\n",
    "    ''' Function splits the input dataframe, by Module, into independent dataframes'''\n",
    "    DF_Input_lst =[]\n",
    "    for Module in data['Module'].unique():\n",
    "        DF=data.loc[data['Module']==Module]\n",
    "        DF_Input_lst.append(DF)\n",
    "    return DF_Input_lst\n",
    "\n",
    "DF_Input_lst=SplitInput_df_byModule(Input)\n",
    "\n",
    "\n",
    "# Importing the Shuffled Mok kinase-Module Score csv files individually and creating dataframes\n",
    "path=r\"Shuffle_KL/\"\n",
    "filenames = glob.glob(path + \"*.csv\")\n",
    "\n",
    "dfs_lst = []\n",
    "shuffledData = load_data(filenames, dfs_lst)\n",
    "shuffledData = pd.DataFrame(dfs_lst, columns=labels)\n",
    "\n",
    "dfs_lst2= SplitInput_df_byModule(shuffledData)\n",
    "\n",
    "\n",
    "def CountScores_Below():\n",
    "    '''Function is calculating the number of scores in the shuffled distribution below a non-shuffled Kinase_Module score '''\n",
    "    Final_DFs_lst=[]   #\n",
    "    for DF_shuffled in dfs_lst2:  \n",
    "        DF_shuffled=DF_shuffled.copy()\n",
    "       \n",
    "        for df_noShuffle in DF_Input_lst:  \n",
    "            df_noShuffle=df_noShuffle.copy()\n",
    "       \n",
    "            if df_noShuffle['Module'].unique().all() == DF_shuffled['Module'].unique().all():      # if all of the values match in the module column for each df, then and only then, perform the below steps\n",
    "                Input_Score_lst=df_noShuffle['Scores'].tolist()  \n",
    "                \n",
    "                Scores_lst=[]  \n",
    "                for score in Input_Score_lst: \n",
    "                    Scores_lst_individual=[]\n",
    "                    num_smaller_items = (DF_shuffled['Scores']<score).sum()                        # create a variable that is the sum of all scores below the score in the Shuffled_Scores dataframe\n",
    "                 \n",
    "                    Scores_lst_individual.append(num_smaller_items)                                # append the number of scores below a given kinase-module score to the individual list.\n",
    "                    Scores_lst.append(Scores_lst_individual)                                       # append the individual scores to a list.\n",
    "                    merged = list(itertools.chain(*Scores_lst))                                    # Flatten the list of lists. \n",
    "                    \n",
    "                df_noShuffle['Counts_Less_Than']=merged\n",
    "                df_noShuffle['Number_of_Scores']=len(DF_shuffled)                                  # take all of the summed scores, one per kinase from the kinase-module no-shuffle dataframe, and create a new column.\n",
    "                Final_DFs_lst.append(df_noShuffle)  \n",
    "    return Final_DFs_lst\n",
    "\n",
    "DFs_with_CountsBelow_lst=CountScores_Below()\n",
    "   \n",
    "def Calculate_FDR():\n",
    "    ''' Function calculates an FDR value by dividing the number of shuffled scores for a kinase-module\n",
    "    that are smaller than the non-shuffled kinase-module score by all shuffled scores (63,000)  '''\n",
    "    DFs_with_CountsBelow_lst2=[]\n",
    "    for DF in DFs_with_CountsBelow_lst:\n",
    "        DF['FDR']=DF['Counts_Less_Than']/DF['Number_of_Scores']\n",
    "        DF=DF.sort(['FDR'], ascending=[True])\n",
    "        DFs_with_CountsBelow_lst2.append(DF)\n",
    "    return DFs_with_CountsBelow_lst2\n",
    " \n",
    "DFs_with_CountsBelow_lst2=Calculate_FDR()\n",
    "\n",
    "# Import the kinases not in the Mok et al dataset.\n",
    "Kinases_Not_In_Mok_DF=pd.read_csv('Kinases_Not_In_Mok.csv')\n",
    "\n",
    "\n",
    "def ConcatenateDFs_with_Kinases_Not_In_Mok():\n",
    "    '''Function adds the kinases not in the Mok et al dataset to the dataframes for each module'''\n",
    "    DFs_with_CountsBelow_lst3=[]\n",
    "    for DF in DFs_with_CountsBelow_lst2:\n",
    "        DF=DF.copy()\n",
    "        FinalDF=DF.append(Kinases_Not_In_Mok_DF)\n",
    "        DFs_with_CountsBelow_lst3.append(FinalDF)\n",
    "    return DFs_with_CountsBelow_lst3\n",
    "\n",
    "DFs_with_CountsBelow_lst3=ConcatenateDFs_with_Kinases_Not_In_Mok() \n",
    "print (DFs_with_CountsBelow_lst3)   \n",
    "\n",
    "def ConcatenateDFs(): \n",
    "    '''Function appends all of the dataframes, for each module, together into one dataframe''' \n",
    "    EmptyDF = pd.DataFrame() \n",
    "    for df in DFs_with_CountsBelow_lst3: \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df) \n",
    "    return EmptyDF\n",
    "\n",
    "Final=ConcatenateDFs()\n",
    "\n",
    "def DF_to_CSV(dataframe, NewFileName): \n",
    "    ''' Write out dataframe as a tab separated file.'''\n",
    "    dataframe.to_csv (NewFileName,sep='\\t') \n",
    "    \n",
    "DF_to_CSV(Final, 'FDR_Scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge_SI_subModule_relationships_with_FDR_Scores\n",
    "\n",
    "Input\n",
    "\n",
    "FDR Scores input file:\n",
    "Scores,Kinase,Group (According to Mok),Module,Counts_Less_Than,Number_of_Scores,FDR\n",
    "11.44097292,pho85-pho80,Proline_directed,Induced_......SP.....,0,63000,0\n",
    "11.92968433,fus3,Proline_directed,Induced_......SP.....,0,63000,0\n",
    "11.02325264,cdc28,Proline_directed,Induced_......SP.....,0,63000,0\n",
    "\n",
    "\n",
    "SI (Shared Interactors file) this cooresponds to the Network_Submodule_Nodes_background_Network.csv above\n",
    "NOTE: the shared interactor name will which is last in the Network_Submodule... file has to be moved to the\n",
    "start of the line.  This is done in the next cell.\n",
    "\n",
    "Shared_Interactor,M,Motif_Containing_Proteins,Motif,n,N,m,p-value,Rank(i),m_(number_of_tests),Q_(FDR),(i/m)Q,BH_significant\n",
    "YBR160W,304,YKL168C,Induced_......SP....._No_Phenotype_Exists,90,4638,24,1.37E-09,1,894,0.05,5.59E-05,1\n",
    "YNL293W,16,YLR319C,Induced_......SP....._mkk1_2_Induced_Defective,31,4638,4,2.81E-06,2,894,0.05,0.000111857,1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prep input files\n",
    "# add group classification according to Mok to FDR_Scores.\n",
    "Mok = dict()\n",
    "with open('Mok_Kinase_Groups_Corrected.csv','r') as mk:\n",
    "    for kns in mk:\n",
    "        kns = kns.rstrip()\n",
    "        group = kns.split()\n",
    "        Mok[group[0]] = group[2]\n",
    "mk.close()\n",
    "\n",
    "grpName = ''\n",
    "\n",
    "with open('FDR_Scores_merged.csv', 'w') as outfile, open('FDR_Scores.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        dat = line.split()\n",
    "        if dat[2] in Mok:\n",
    "            grpName = dat[3]\n",
    "            dat.insert(3,Mok[dat[2]])\n",
    "            out = ','.join(dat) + '\\n'\n",
    "            outfile.write(out)\n",
    "        else:\n",
    "            dat.insert(3, grpName)\n",
    "            out =','.join(dat) + '\\n'\n",
    "            outfile.write(out)\n",
    "            \n",
    "outfile.close()\n",
    "f.close()\n",
    "\n",
    "# Reorder Shared interactors w/ in the line, file Network_Submodule_Nodes_background_Network.csv\n",
    "with open('Network_Submodule_Nodes_background_Network.csv','r') as f, open('All_SIs.csv','w') as outfile:\n",
    "    for line in f:\n",
    "        dat = line.rstrip().split(',')\n",
    "        if line.startswith('M'):\n",
    "            dat[1] = 'Motif'\n",
    "        last = dat.pop(-1)\n",
    "        dat.insert(0,last)\n",
    "        out = ','.join(dat) + '\\n'\n",
    "        outfile.write(out)\n",
    "        \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' This script takes the output of the KLD FDR script and adds these FDR scores to the identified SI-submodule pairs. \n",
    "'''\n",
    "\n",
    "FDR_Scores_DF=pd.read_csv('FDR_Input_T120_ForMerge_withSIs.csv')  # All FDR scores for Mok kinase and module PWM comparison\n",
    "SIs_DF=pd.read_csv('DTT_T120_SIs_All_Sept2017.csv')  # SIs - All, enriched and not enriched\n",
    "Kinase_Names_DF=pd.read_csv('Kinases_Mok_andNOT_In_Mok.csv') # Contains all kinases in Mok and not in Mok. Contains common names and YORFs, also contains modifications for Pho85 naming (ex: Pho85-Pcl is now Pho85 in one column)\n",
    "\n",
    "def DF_to_CSV(dataframe, NewFileName):  \n",
    "    dataframe.to_csv (current_dir + '/' + NewFileName,sep='\\t') \n",
    "####################################################################################################################################\n",
    "''' Merging the FDR_Scores_DF with the Kinase_Names_DF '''\n",
    "# This step is completed so that the correct Pho85 nomenclature is used for a subsquent merge with the SI dataframe. This is necessary because there are 3 pho85-cofactor variants in the Mok et al, dataset.\n",
    "\n",
    "FDR_Scores_DF_merged_left = pd.merge(left=FDR_Scores_DF,right=Kinase_Names_DF, how='left', left_on='Kinase', right_on='Kinase') # completing a merge so that all the kinase nomenclature from the Kinase_Names_DF\n",
    "\n",
    "####################################################################################################################################\n",
    "''' Merge the Kinase name (common name,ex. Hog1) with the Module name to create a new column called \"Candidate_Kinase_Regulators\" '''\n",
    "\n",
    "FDR_Scores_DF_merged_left['Candidate_Kinase_Regulators'] = FDR_Scores_DF_merged_left.Module.map(str) + \"_\" + FDR_Scores_DF_merged_left.Kinase_Pho85_renamed # creating a new column that is the result of a merge between Kinase_Pho85_renamed, and Module\n",
    "#print (FDR_Scores_DF_merged_left.head(1))\n",
    "DF_to_CSV(FDR_Scores_DF_merged_left, 'yay3.csv')\n",
    "####################################################################################################################################\n",
    "''' Filtering out non-enriched SIs'''\n",
    "\n",
    "SIs_Filtered=SIs_DF.loc[SIs_DF['BH_significant'] == 1] # Filter out non-significant shared interactors (anything with a \"0\") \n",
    "\n",
    "####################################################################################################################################\n",
    "''' Adding to the SIs file, the \"common\" name for the proteins, rather than just using the YORF designation'''\n",
    "SIs_Filtered_merged_left= pd.merge(left=SIs_Filtered,right=Kinase_Names_DF, how='left', left_on='Shared_Interactor', right_on='Kinase_YORF')\n",
    "\n",
    "####################################################################################################################################\n",
    "''' Splitting 'Motif' column and producing a new column, called \"Module\" that only lists the Induced/Repressed WT phenotype and the motif '''\n",
    "\n",
    "def Split_After_2nd_Occurence_In_A_String_Retaining_Beginning():\n",
    "    lst=[] # create an empty list \n",
    "    for string in SIs_Filtered_merged_left['Motif']: # Select the string from the \"Motif\" column \n",
    "        strip_character =\"_\"  # define character where strip will occur\n",
    "        lst.append(strip_character.join(string.split(strip_character)[:2])) # append to the list the text before the second occurence of the character \"_\"\n",
    "    Series_Object = pd.Series(lst) # put the list into a series \n",
    "    SIs_Filtered_merged_left['Module'] = Series_Object.values # append the series values to the already existing DF in a new column\n",
    "    return SIs_Filtered_merged_left\n",
    "        \n",
    "SIs_Filtered_merged_left_String_Split=Split_After_2nd_Occurence_In_A_String_Retaining_Beginning()\n",
    "#print (SIs_Filtered_merged_left_String_Split)\n",
    "\n",
    "####################################################################################################################################    \n",
    "####################################################################################################################################    \n",
    "'''Creating New Columns that can be used for a merge'''\n",
    "SIs_Filtered_merged_left_String_Split['Kinase_subModules'] = SIs_Filtered_merged_left_String_Split.Motif.map(str) + \"_\" + SIs_Filtered_merged_left_String_Split.Kinase_Pho85_renamed\n",
    "\n",
    "\n",
    "SIs_Filtered_merged_left_String_Split['Kinase_Modules'] = SIs_Filtered_merged_left_String_Split.Module.map(str) + \"_\" + SIs_Filtered_merged_left_String_Split.Kinase_Pho85_renamed\n",
    "\n",
    "#################################################################################################################################### \n",
    "''' Perform a merge where of the FDR Scores Dataframe with the SIs_Filtered_merged_left_String_Split DF. \n",
    "This will reveal if a kinase-Module relationship, from the FDR Score Dataframe, which contains all possible Kinase-Module relationships, exist in the users \n",
    "kinase-subModule file (so the SIs file)'''\n",
    "    \n",
    "merged_left= pd.merge(left=FDR_Scores_DF_merged_left,right=SIs_Filtered_merged_left_String_Split, how='left', left_on='Candidate_Kinase_Regulators', right_on='Kinase_Modules')\n",
    "\n",
    "#################################################################################################################################### \n",
    "''' Drop columns that are not needed or redundant '''\n",
    "\n",
    "merged_left=merged_left[['Scores', 'Kinase_x', 'Module_x', 'Candidate_Kinase_Regulators', 'Counts_Less_Than', 'Number_of_Scores', 'FDR', 'Kinase_subModules']]\n",
    "\n",
    "#################################################################################################################################### \n",
    "''' Drop NaN values '''\n",
    "merged_left=merged_left.dropna(subset=['Kinase_subModules']) # Drop the NaN values, so that the dataframe only contains Kinases that were connected to subModules.\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "''' Drop any duplicates that occur in TWO columns - this is done only because of Pho85 being listed 3 times (because of it's co-factor interactions) and that affects the merge'''\n",
    "merged_left=merged_left.drop_duplicates(subset=['Kinase_x', 'Kinase_subModules']) # only drop duplicates that are found in BOTH columns\n",
    "\n",
    "DF_to_CSV(merged_left, 'All_FDR_Scores_and_their_Kinase_SI_subModules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores                         float64\n",
      "Kinase_x                        object\n",
      "Module_x                        object\n",
      "Candidate_Kinase_Regulators     object\n",
      "Counts_Less_Than                 int64\n",
      "Number_of_Scores                 int64\n",
      "FDR                            float64\n",
      "Kinase_subModules               object\n",
      "dtype: object\n",
      "       Scores     Kinase_x                 Module_x  \\\n",
      "0   11.023253        cdc28    Induced_......SP.....   \n",
      "1   11.023253        cdc28    Induced_......SP.....   \n",
      "2   12.515798         slt2    Induced_......SP.....   \n",
      "3   12.515798         slt2    Induced_......SP.....   \n",
      "4   13.036719         hog1    Induced_......SP.....   \n",
      "5   15.742683         tpk1    Induced_......SP.....   \n",
      "6   15.829279        rad53    Induced_......SP.....   \n",
      "7   15.859105      ynr047w    Induced_......SP.....   \n",
      "8   16.077946         kin1    Induced_......SP.....   \n",
      "9   16.580668      ydl025c    Induced_......SP.....   \n",
      "10  17.769782         ptk2    Induced_......SP.....   \n",
      "11  18.632372        cdc15    Induced_......SP.....   \n",
      "12  19.909495         cmk2    Induced_......SP.....   \n",
      "13  10.884312         yck2    Induced_......TP.....   \n",
      "14  10.981367         ptk2    Induced_......TP.....   \n",
      "15  11.316459        hrr25    Induced_......TP.....   \n",
      "16  11.918735         ark1    Induced_......TP.....   \n",
      "17  12.275687         akl1    Induced_......TP.....   \n",
      "18  13.244184         pkh2    Induced_......TP.....   \n",
      "19   6.235065         rck2    Induced_....R.S......   \n",
      "20   6.536869         yck1    Induced_....R.S......   \n",
      "21   7.347012         ark1    Induced_....R.S......   \n",
      "22   7.513954         atg1    Induced_....R.S......   \n",
      "23   7.735237   pho85-pcl2    Induced_....R.S......   \n",
      "24   7.816351  pho85-pho80    Induced_....R.S......   \n",
      "25   8.083735         ptk2    Induced_....R.S......   \n",
      "26   8.871182   pho85-pcl1    Induced_....R.S......   \n",
      "27   8.880341         fus3    Induced_....R.S......   \n",
      "28  12.332135         mek1    Induced_...K..S......   \n",
      "29  10.350127  pho85-pho80    Induced_...K..SP.....   \n",
      "..        ...          ...                      ...   \n",
      "32  11.080363   pho85-pcl1    Induced_...K..SP.....   \n",
      "33  13.320784        rad53    Induced_...K..SP.....   \n",
      "34  13.448153         tos3    Induced_...K..SP.....   \n",
      "35  14.177043         atg1    Induced_...K..SP.....   \n",
      "36  15.859416         cmk1    Induced_...K..SP.....   \n",
      "37  13.056535         tpk1    Induced_...R..S......   \n",
      "38  12.724714         rck2    Induced_...R..S......   \n",
      "39  11.896832         tpk2    Induced_...R..S......   \n",
      "40  16.565692         ptk2    Induced_...R..S......   \n",
      "41  17.190434        hrr25    Induced_...R..S......   \n",
      "42  18.822166        cdc28    Induced_...R..S......   \n",
      "43  19.379879         mck1    Induced_...R..S......   \n",
      "44   7.403009         tpk3    Induced_...RR.S......   \n",
      "45   7.411970         tpk2    Induced_...RR.S......   \n",
      "46   7.411970         tpk2    Induced_...RR.S......   \n",
      "47   8.627443         rck2    Induced_...RR.S......   \n",
      "48   7.844100         tpk1    Induced_...RR.S......   \n",
      "49   7.844100         tpk1    Induced_...RR.S......   \n",
      "50   7.844100         tpk1    Induced_...RR.S......   \n",
      "51   9.913233         sky1    Induced_...RR.S......   \n",
      "52  10.966090         tos3    Induced_...RR.S......   \n",
      "53  11.635049         hsl1    Induced_...RR.S......   \n",
      "54  11.674675        rad53    Induced_...RR.S......   \n",
      "55  11.681877         ptk2    Induced_...RR.S......   \n",
      "56  11.681877         ptk2    Induced_...RR.S......   \n",
      "57  11.681877         ptk2    Induced_...RR.S......   \n",
      "58  11.683399         cmk1    Induced_...RR.S......   \n",
      "59   6.107491        cdc28  Repressed_......TP.....   \n",
      "60  10.701029        rad53  Repressed_...R..S......   \n",
      "61  11.023895         ark1  Repressed_...R..S......   \n",
      "\n",
      "      Candidate_Kinase_Regulators  Counts_Less_Than  Number_of_Scores  \\\n",
      "0     Induced_......SP....._cdc28                 0             63000   \n",
      "1     Induced_......SP....._cdc28                 0             63000   \n",
      "2      Induced_......SP....._slt2                 1             63000   \n",
      "3      Induced_......SP....._slt2                 1             63000   \n",
      "4      Induced_......SP....._hog1                 2             63000   \n",
      "5      Induced_......SP....._tpk1               179             63000   \n",
      "6     Induced_......SP....._rad53               193             63000   \n",
      "7   Induced_......SP....._ynr047w               204             63000   \n",
      "8      Induced_......SP....._kin1               253             63000   \n",
      "9   Induced_......SP....._ydl025c               369             63000   \n",
      "10     Induced_......SP....._ptk2               749             63000   \n",
      "11    Induced_......SP....._cdc15              1115             63000   \n",
      "12     Induced_......SP....._cmk2              4293             63000   \n",
      "13     Induced_......TP....._yck2               208             63000   \n",
      "14     Induced_......TP....._ptk2               234             63000   \n",
      "15    Induced_......TP....._hrr25               350             63000   \n",
      "16     Induced_......TP....._ark1               574             63000   \n",
      "17     Induced_......TP....._akl1               778             63000   \n",
      "18     Induced_......TP....._pkh2              2569             63000   \n",
      "19     Induced_....R.S......_rck2                 2             63000   \n",
      "20     Induced_....R.S......_yck1                 8             63000   \n",
      "21     Induced_....R.S......_ark1               168             63000   \n",
      "22     Induced_....R.S......_atg1               222             63000   \n",
      "23    Induced_....R.S......_pho85               278             63000   \n",
      "24    Induced_....R.S......_pho85               312             63000   \n",
      "25     Induced_....R.S......_ptk2               425             63000   \n",
      "26    Induced_....R.S......_pho85              1135             63000   \n",
      "27     Induced_....R.S......_fus3              1153             63000   \n",
      "28     Induced_...K..S......_mek1               777             63000   \n",
      "29    Induced_...K..SP....._pho85                 0             63000   \n",
      "..                            ...               ...               ...   \n",
      "32    Induced_...K..SP....._pho85                 4             63000   \n",
      "33    Induced_...K..SP....._rad53               284             63000   \n",
      "34     Induced_...K..SP....._tos3               338             63000   \n",
      "35     Induced_...K..SP....._atg1               688             63000   \n",
      "36     Induced_...K..SP....._cmk1              2863             63000   \n",
      "37     Induced_...R..S......_tpk1                 0             63000   \n",
      "38     Induced_...R..S......_rck2                 0             63000   \n",
      "39     Induced_...R..S......_tpk2                 0             63000   \n",
      "40     Induced_...R..S......_ptk2                27             63000   \n",
      "41    Induced_...R..S......_hrr25               109             63000   \n",
      "42    Induced_...R..S......_cdc28               486             63000   \n",
      "43     Induced_...R..S......_mck1               689             63000   \n",
      "44     Induced_...RR.S......_tpk3                 0             63000   \n",
      "45     Induced_...RR.S......_tpk2                 0             63000   \n",
      "46     Induced_...RR.S......_tpk2                 0             63000   \n",
      "47     Induced_...RR.S......_rck2                 0             63000   \n",
      "48     Induced_...RR.S......_tpk1                 0             63000   \n",
      "49     Induced_...RR.S......_tpk1                 0             63000   \n",
      "50     Induced_...RR.S......_tpk1                 0             63000   \n",
      "51     Induced_...RR.S......_sky1                 0             63000   \n",
      "52     Induced_...RR.S......_tos3                 4             63000   \n",
      "53     Induced_...RR.S......_hsl1                22             63000   \n",
      "54    Induced_...RR.S......_rad53                24             63000   \n",
      "55     Induced_...RR.S......_ptk2                25             63000   \n",
      "56     Induced_...RR.S......_ptk2                25             63000   \n",
      "57     Induced_...RR.S......_ptk2                25             63000   \n",
      "58     Induced_...RR.S......_cmk1                25             63000   \n",
      "59  Repressed_......TP....._cdc28                 3             63000   \n",
      "60  Repressed_...R..S......_rad53                 7             63000   \n",
      "61   Repressed_...R..S......_ark1                18             63000   \n",
      "\n",
      "         FDR                                  Kinase_subModules  \n",
      "0   0.000000          Induced_......SP....._No_Phenotype_Exists  \n",
      "1   0.000000     Induced_......SP....._mkk1_2_Induced_Defective  \n",
      "2   0.000016     Induced_......SP....._mkk1_2_Induced_Defective  \n",
      "3   0.000016          Induced_......SP....._No_Phenotype_Exists  \n",
      "4   0.000032          Induced_......SP....._No_Phenotype_Exists  \n",
      "5   0.002841          Induced_......SP....._No_Phenotype_Exists  \n",
      "6   0.003063          Induced_......SP....._No_Phenotype_Exists  \n",
      "7   0.003238          Induced_......SP....._No_Phenotype_Exists  \n",
      "8   0.004016          Induced_......SP....._No_Phenotype_Exists  \n",
      "9   0.005857     Induced_......SP....._mkk1_2_Induced_Defective  \n",
      "10  0.011889          Induced_......SP....._No_Phenotype_Exists  \n",
      "11  0.017698          Induced_......SP....._No_Phenotype_Exists  \n",
      "12  0.068143          Induced_......SP....._No_Phenotype_Exists  \n",
      "13  0.003302     Induced_......TP....._mkk1_2_Induced_Defective  \n",
      "14  0.003714     Induced_......TP....._mkk1_2_Induced_Defective  \n",
      "15  0.005556          Induced_......TP....._No_Phenotype_Exists  \n",
      "16  0.009111          Induced_......TP....._No_Phenotype_Exists  \n",
      "17  0.012349          Induced_......TP....._No_Phenotype_Exists  \n",
      "18  0.040778          Induced_......TP....._No_Phenotype_Exists  \n",
      "19  0.000032          Induced_....R.S......_No_Phenotype_Exists  \n",
      "20  0.000127          Induced_....R.S......_No_Phenotype_Exists  \n",
      "21  0.002667          Induced_....R.S......_No_Phenotype_Exists  \n",
      "22  0.003524          Induced_....R.S......_No_Phenotype_Exists  \n",
      "23  0.004413          Induced_....R.S......_No_Phenotype_Exists  \n",
      "24  0.004952          Induced_....R.S......_No_Phenotype_Exists  \n",
      "25  0.006746          Induced_....R.S......_No_Phenotype_Exists  \n",
      "26  0.018016          Induced_....R.S......_No_Phenotype_Exists  \n",
      "27  0.018302          Induced_....R.S......_No_Phenotype_Exists  \n",
      "28  0.012333          Induced_...K..S......_No_Phenotype_Exists  \n",
      "29  0.000000     Induced_...K..SP....._mkk1_2_Induced_Defective  \n",
      "..       ...                                                ...  \n",
      "32  0.000063     Induced_...K..SP....._mkk1_2_Induced_Defective  \n",
      "33  0.004508          Induced_...K..SP....._No_Phenotype_Exists  \n",
      "34  0.005365     Induced_...K..SP....._mkk1_2_Induced_Defective  \n",
      "35  0.010921     Induced_...K..SP....._mkk1_2_Induced_Defective  \n",
      "36  0.045444     Induced_...K..SP....._mkk1_2_Induced_Defective  \n",
      "37  0.000000          Induced_...R..S......_No_Phenotype_Exists  \n",
      "38  0.000000          Induced_...R..S......_No_Phenotype_Exists  \n",
      "39  0.000000          Induced_...R..S......_No_Phenotype_Exists  \n",
      "40  0.000429          Induced_...R..S......_No_Phenotype_Exists  \n",
      "41  0.001730          Induced_...R..S......_No_Phenotype_Exists  \n",
      "42  0.007714          Induced_...R..S......_No_Phenotype_Exists  \n",
      "43  0.010937          Induced_...R..S......_No_Phenotype_Exists  \n",
      "44  0.000000       Induced_...RR.S......_ire1_Induced_Defective  \n",
      "45  0.000000       Induced_...RR.S......_ire1_Induced_Defective  \n",
      "46  0.000000     Induced_...RR.S......_mkk1_2_Induced_Defective  \n",
      "47  0.000000     Induced_...RR.S......_mkk1_2_Induced_Defective  \n",
      "48  0.000000     Induced_...RR.S......_mkk1_2_Induced_Defective  \n",
      "49  0.000000       Induced_...RR.S......_ire1_Induced_Defective  \n",
      "50  0.000000          Induced_...RR.S......_No_Phenotype_Exists  \n",
      "51  0.000000          Induced_...RR.S......_No_Phenotype_Exists  \n",
      "52  0.000063     Induced_...RR.S......_mkk1_2_Induced_Defective  \n",
      "53  0.000349     Induced_...RR.S......_mkk1_2_Induced_Defective  \n",
      "54  0.000381          Induced_...RR.S......_No_Phenotype_Exists  \n",
      "55  0.000397          Induced_...RR.S......_No_Phenotype_Exists  \n",
      "56  0.000397     Induced_...RR.S......_mkk1_2_Induced_Defective  \n",
      "57  0.000397       Induced_...RR.S......_ire1_Induced_Defective  \n",
      "58  0.000397          Induced_...RR.S......_No_Phenotype_Exists  \n",
      "59  0.000048        Repressed_......TP....._No_Phenotype_Exists  \n",
      "60  0.000111  Repressed_...R..S......_mkk1_2_Repressed_Defec...  \n",
      "61  0.000286        Repressed_...R..S......_No_Phenotype_Exists  \n",
      "\n",
      "[62 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mplace/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:42: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "''' This is a quick script that cleans up the Output of the 3_Kullback_Leibler Shuffle Script. It removes unwanted names that trail\n",
    "the subModule name-these are leftovers from a previous script. It then also sorts each subModule by ascending for the SI scores\n",
    "\n",
    "example input file: All_Mok_Kinases_Shuffled1000x_Newest_Method_Compared_17Modules_FDR_Scores_SIs_andScores.csv\n",
    "example output file :All_Mok_Kinases_Shuffled1000x_Newest_Method_Compared_17Modules_FDR_Scores_SIs_andScores_Sorted_Ascending.csv '''\n",
    "\n",
    "Input=pd.read_csv('All_DTT_T120_Kinase_Module_FDR_Scores_and_their_Kinase_SI_subModules_Sept2017.csv')\n",
    "print (Input.dtypes)\n",
    "\n",
    "# Remove the last occurrence of a character, and the text that follows\n",
    "def Remove_Text_After_Last_Occurence_of_Character():\n",
    "    Value_lst=[]\n",
    "    for value in Input['Kinase_subModules']:\n",
    "        value=\"_\".join(value.split(\"_\")[:-1]) # return everything minus the last occurrence of the \"_\" and what trailed\n",
    "        Value_lst.append(value)\n",
    "    Input['Kinase_subModules']=Value_lst\n",
    "        #sep = '_'\n",
    "        #value = value.split(sep, 5)[-1]\n",
    "    return (Input)\n",
    "        \n",
    "        \n",
    "Input=Remove_Text_After_Last_Occurence_of_Character()\n",
    "print (Input)\n",
    "\n",
    "#Split the dataframe into separate dataframes by the subModule name\n",
    "def SplitInput_df_by_subModule():\n",
    "    DF_Input_lst =[]\n",
    "    for subModule in Input['Kinase_subModules'].unique():\n",
    "        DF=Input.loc[Input['Kinase_subModules']==subModule]\n",
    "        DF_Input_lst.append(DF)\n",
    "    return DF_Input_lst\n",
    "\n",
    "DF_Input_lst=SplitInput_df_by_subModule()\n",
    "\n",
    "#Sort each dataframe within the list of dataframes by ascending for the FDR column\n",
    "def Sort_by_Ascending():\n",
    "    DF_Input_lst2=[]\n",
    "    for DF in DF_Input_lst:\n",
    "        DF=DF.copy()\n",
    "        DF=DF.sort(['FDR'], ascending=[True])\n",
    "        DF_Input_lst2.append(DF)\n",
    "    return DF_Input_lst2\n",
    "\n",
    "DF_Input_lst2=Sort_by_Ascending()\n",
    "\n",
    "\n",
    "\n",
    "#Concatenate the dataframes back together into one so they can be printed out as a single dataframe.\n",
    "def ConcatenateDFs():    #Concatenate the DFs together \n",
    "    EmptyDF = pd.DataFrame() # create an empty dataframe\n",
    "    for df in DF_Input_lst2:  # select a dataframe in the list \n",
    "        df=df.copy() # make a copy of that dataframe \n",
    "        EmptyDF=EmptyDF.append(df) # append to the empty DF the dataframe selected and overwrite the empty dataframe\n",
    "    return EmptyDF\n",
    "\n",
    "Final=ConcatenateDFs()\n",
    "#print (Final)\n",
    "\n",
    "\n",
    "def DF_to_CSV(dataframe, NewFileName): \n",
    "    dataframe.to_csv (NewFileName,sep='\\t')\n",
    "    \n",
    "DF_to_CSV(Final, 'All_DTT_T120_Kinase_Module_FDR_Scores_and_their_Kinase_SI_subModules_Sorted_by_Ascending_Sept2017.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
